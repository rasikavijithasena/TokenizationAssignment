{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rasika_105127\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from', 'Canada', '.', '#cdnpoli', '#LPC', '#CPCLDR', '�', '�', '_', 'https://t.co/ZOZOSe1CqQ', '#immigration', '#integration', '#canada', 'https://t.co/M5cKGyvV8F', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'UK', 'economy', '.', 'Same', 'as', 'Australia', '&', 'Canada', '.', 'https://t.co/99mYliuOes', 'Is', 'the', 'new', 'Manitoba', 'immigration', 'fee', 'a', 'head', 'tax', '?', 'https://t.co/LsG7C3vLe9', 'Canada', 'immigration', 'profit', 'influence', 'modernistic', 'delhi', 'yet', 'abhinav', ':', 'XKofy', 'https://t.co/becgusY2i6', 'Canada', 'Immigration', 'Minister', 'to', '�', '�', '�', 'Substantially', 'Increase', 'Immigration', 'Numbers', 'https://t.co/nEFw30MRaa', 'https://t.co/cyI867PZRV', 'M', '�', '�', 'me', 'les', '#USA', '=p', 'ays', \"d'immigration\", 'par', 'excellence', 'CONTR', '�', '�', 'LE', 'RIGOUREUSEMENT', \"l'immigration\", 'et', 'acc', '�', '�', 's', '�', '�', 'la', '#GreenCARD', '!', '�', '�', '_', 'https://t.co/IHpVhW2BaG', '@Shawhelp', 'what', 'changes', 'should', 'be', 'made', 'to', \"Canada's\", 'immigration', 'laws', 'due', 'to', 'the', 'influx', 'of', 'immigration', 'and', 'violence', '?', 'L', '�', '�', 'immigration', 'irr', '�', '�', 'guli', '�', '�', 're', 'au', 'Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', 'questions', 'https://t.co/f4utO5A7ZF', \"L'immigration\", 'irr', '�', '�', 'guli', '�', '�', 're', 'au', 'Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', 'questions', '-', 'https://t.co/UiBsEZOqas', 'https://t.co/j77dEvjoiX', 'https://t.co/XXDeIG7Dbu', 'Will', 'Media', 'ask', 'the', 'Liberals', 'if', 'they', 'actually', 'have', 'a', 'solid', 'plan', 'for', 'Canada', '_', '�', '�', '_', '�', '_', '?', '?', 'From', 'my', 'view', '-', '-', 'immigration', 'out', 'of', 'C', '�', '�', '_', 'https://t.co/YAgwmZ8ECp', 'Dan', 'Murray', 'of', '�', '�', 'Immigration', 'Watch', 'Canada', 'is', 'xenophobic', 'racist', 'fear-mongering', 'liar', '#racism', '#canada', '#cdnpoli', '#hatecrime', '�', '�', '_', 'https://t.co/kwZ3csvYxM', 'Le', 'Canada', 'lance', 'une', 'vaste', 'campagne', \"d'immigration\", 'pour', 'faire', 'face', '�', '�', 'son', 'besoin', 'de', 'main', 'd', \"'\", '�', '�', 'uvre', 'https://t.co/kXdfMGTZzN', 'L', '�', '�', '#immigration', 'irr', '�', '�', 'guli', '�', '�', 're', 'au', '#Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', '�', '�', 'questions', 'https://t.co/s3hu1OKKIG', '@Canadidly', \"I've\", 'read', 'the', 'Immigration', 'laws', 'of', 'Canada', 'much', 'stricter', 'than', 'the', 'US', 'Canada', 'Immigration', 'Website', 'Traffic', 'Surges', 'And', 'Crashes', 'In', 'Wake', 'Of', 'Trump', '#fasttraffic', ',', '#sitetraffic', ',', '#website', ',', '#traffic', 'https://t.co/zRlJ26jnkC', 'Mr', 'Know-all', 'of', 'Canada', 'Immigration', 'https://t.co/wTQK4QDiKI', 'Move', 'to', 'Canada', '@LadyMadonna___', 'Oh', ',', 'immigration', 'rules', ',', 'you', \"can't\", '...', 'https://t.co/5LIEVHO7A4', '#OnThisDay', 'Annette', 'Toft', 'becomes', \"Canada's\", '2', 'millionth', 'immigrant', 'since', '1945', '.', 'Do', 'you', 'know', 'your', \"family's\", 'immigration', 'st', '�', '�', '_', 'https://t.co/UvRuw8eR1b', '.', '@TheEconomist', 'profiles', \"Canada's\", 'open', 'immigration', 'policies', '&', 'how', 'they', 'contribute', 'to', 'our', 'economic', 'success', ':', '�', '�', '_', 'https://t.co/4K84EE8Y63', 'Hundreds', 'may', 'lose', 'Canadian', 'citizenship', ',', 'resident', 'status', 'because', 'of', 'one', 'corrupt', 'immigration', 'consultant', 'https://t.co/x2IfO0EXI2', 'Immigration', 'for', 'canada', 'without', 'india', ':', 'an', 'compassionate', 'handle', ':', 'deyFy', '\"', '#Jamaican', '#immigrants', '#Canada', 'https://t.co/vcmfYGadR5', '#statistics', '#immigration', '\"', 'Mexican', 'visa', 'lift', 'expected', 'to', 'cost', 'Canada', '$', '262M', 'over', 'a', 'decade', 'https://t.co/9i72fRhtij', 'Are', 'people', 'still', 'moving', 'to', '#Canada', '?', '?', '?', 'Oh', \"that's\", 'right', ',', 'they', 'have', 'real', 'immigration', 'laws', 'and', \"it's\", '�', '�', '_', 'https://t.co/0C5OBfmxLG', 'Here', 'are', 'more', 'details', 'on', 'the', 'Richmond', ',', 'B', '.', 'C', '.', 'Immigration', 'Consultant', 'Sunny', 'Wang', 'who', 'was', 'sentenced', 'to', '7', 'years', 'in', '...', 'https://t.co/YXH5W53srO', 'I', 'added', 'a', 'video', 'to', 'a', '@YouTube', 'playlist', 'https://t.co/CnEyWN40x3', 'Funny', 'Talking', 'of', 'Haryanavi', 'Jat', 'with', 'Canada', 'Immigration', 'Girl', 'Agent', 'Mexicans', 'Can', 'Now', 'Travel', 'Visa-Free', 'To', 'Canada', 'https://t.co/Ec3XHORO2s', 'https://t.co/RQRr5nebcG', 'L', '�', '�', 'immigration', 'irr', '�', '�', 'guli', '�', '�', 're', 'au', 'Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', '�', '�', 'questions', 'https://t.co/DkpuKyWmaK', '@SweetnessShawnB', 'Hes', 'the', 'POS', 'that', 'ramped', 'up', 'immigration', 'for', 'Canada', ',', 'among', 'other', 'globalist', 'policies', '.', 'Canada', 'lifted', 'visa', 'requirements', 'to', 'Mexico', 'as', 'of', 'Dec', '1', ',', '2016', '.', 'Thoughts', '?', '#visa', '#immigration', '@HuffingtonPost', 'people', 'Keep', 'praising', 'Canada', 'and', 'Canada', 'has', 'way', 'stricter', 'immigration', 'laws', 'then', 'us', 'they', 'willl', 'boot', 'your', 'liberal', 'American', 'ass']\n"
     ]
    }
   ],
   "source": [
    "#read from file\n",
    "fileObjectTwitter = open(\"twitter.txt\", \"r\", encoding='utf-8')\n",
    "twitterdata = fileObjectTwitter.read()\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "twitterTokens = tweet_tokenizer.tokenize(twitterdata)\n",
    "\n",
    "print(twitterTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from', 'Canada', '.', '#', 'cdnpoli', '#', 'LPC', '#', 'CPCLDR��_', 'https', ':', '//t.co/ZOZOSe1CqQ', '#', 'immigration', '#', 'integration', '#', 'canada', 'https', ':', '//t.co/M5cKGyvV8F', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'UK', 'economy', '.', 'Same', 'as', 'Australia', '&', 'amp', ';', 'Canada', '.', 'https', ':', '//t.co/99mYliuOes', 'Is', 'the', 'new', 'Manitoba', 'immigration', 'fee', 'a', 'head', 'tax', '?', 'https', ':', '//t.co/LsG7C3vLe9', 'Canada', 'immigration', 'profit', 'influence', 'modernistic', 'delhi', 'yet', 'abhinav', ':', 'XKofy', 'https', ':', '//t.co/becgusY2i6', 'Canada', 'Immigration', 'Minister', 'to', '���Substantially', 'Increase', 'Immigration', 'Numbers', 'https', ':', '//t.co/nEFw30MRaa', 'https', ':', '//t.co/cyI867PZRV', 'M��me', 'les', '#', 'USA=pays', \"d'immigration\", 'par', 'excellence', 'CONTR��LE', 'RIGOUREUSEMENT', \"l'immigration\", 'et', 'acc��s', '��', 'la', '#', 'GreenCARD', '!', '��_', 'https', ':', '//t.co/IHpVhW2BaG', '@', 'Shawhelp', 'what', 'changes', 'should', 'be', 'made', 'to', 'Canada', \"'s\", 'immigration', 'laws', 'due', 'to', 'the', 'influx', 'of', 'immigration', 'and', 'violence', '?', 'L��immigration', 'irr��guli��re', 'au', 'Canada', 'd��cortiqu��e', 'en', '5', 'questions', 'https', ':', '//t.co/f4utO5A7ZF', \"L'immigration\", 'irr��guli��re', 'au', 'Canada', 'd��cortiqu��e', 'en', '5', 'questions', '-', 'https', ':', '//t.co/UiBsEZOqas', 'https', ':', '//t.co/j77dEvjoiX', 'https', ':', '//t.co/XXDeIG7Dbu', 'Will', 'Media', 'ask', 'the', 'Liberals', 'if', 'they', 'actually', 'have', 'a', 'solid', 'plan', 'for', 'Canada', '_��_�_', '?', '?', 'From', 'my', 'view', '--', 'immigration', 'out', 'of', 'C��_', 'https', ':', '//t.co/YAgwmZ8ECp', 'Dan', 'Murray', 'of��Immigration', 'Watch', 'Canada', 'is', 'xenophobic', 'racist', 'fear-mongering', 'liar', '#', 'racism', '#', 'canada', '#', 'cdnpoli', '#', 'hatecrime��_', 'https', ':', '//t.co/kwZ3csvYxM', 'Le', 'Canada', 'lance', 'une', 'vaste', 'campagne', \"d'immigration\", 'pour', 'faire', 'face', '��', 'son', 'besoin', 'de', 'main', \"d'��uvre\", 'https', ':', '//t.co/kXdfMGTZzN', 'L��', '#', 'immigration', 'irr��guli��re', 'au', '#', 'Canada', 'd��cortiqu��e', 'en', '5��questions', 'https', ':', '//t.co/s3hu1OKKIG', '@', 'Canadidly', 'I', \"'ve\", 'read', 'the', 'Immigration', 'laws', 'of', 'Canada', 'much', 'stricter', 'than', 'the', 'US', 'Canada', 'Immigration', 'Website', 'Traffic', 'Surges', 'And', 'Crashes', 'In', 'Wake', 'Of', 'Trump', '#', 'fasttraffic', ',', '#', 'sitetraffic', ',', '#', 'website', ',', '#', 'traffic', 'https', ':', '//t.co/zRlJ26jnkC', 'Mr', 'Know-all', 'of', 'Canada', 'Immigration', 'https', ':', '//t.co/wTQK4QDiKI', 'Move', 'to', 'Canada', '@', 'LadyMadonna___', 'Oh', ',', 'immigration', 'rules', ',', 'you', 'ca', \"n't\", '...', 'https', ':', '//t.co/5LIEVHO7A4', '#', 'OnThisDay', 'Annette', 'Toft', 'becomes', 'Canada', \"'s\", '2', 'millionth', 'immigrant', 'since', '1945', '.', 'Do', 'you', 'know', 'your', 'family', \"'s\", 'immigration', 'st��_', 'https', ':', '//t.co/UvRuw8eR1b', '.', '@', 'TheEconomist', 'profiles', 'Canada', \"'s\", 'open', 'immigration', 'policies', '&', 'amp', ';', 'how', 'they', 'contribute', 'to', 'our', 'economic', 'success', ':', '��_', 'https', ':', '//t.co/4K84EE8Y63', 'Hundreds', 'may', 'lose', 'Canadian', 'citizenship', ',', 'resident', 'status', 'because', 'of', 'one', 'corrupt', 'immigration', 'consultant', 'https', ':', '//t.co/x2IfO0EXI2', 'Immigration', 'for', 'canada', 'without', 'india', ':', 'an', 'compassionate', 'handle', ':', 'deyFy', \"''\", '#', 'Jamaican', '#', 'immigrants', '#', 'Canada', 'https', ':', '//t.co/vcmfYGadR5', '#', 'statistics', '#', 'immigration', \"''\", 'Mexican', 'visa', 'lift', 'expected', 'to', 'cost', 'Canada', '$', '262M', 'over', 'a', 'decade', 'https', ':', '//t.co/9i72fRhtij', 'Are', 'people', 'still', 'moving', 'to', '#', 'Canada', '?', '?', '?', 'Oh', 'that', \"'s\", 'right', ',', 'they', 'have', 'real', 'immigration', 'laws', 'and', \"it's��_\", 'https', ':', '//t.co/0C5OBfmxLG', 'Here', 'are', 'more', 'details', 'on', 'the', 'Richmond', ',', 'B.C', '.', 'Immigration', 'Consultant', 'Sunny', 'Wang', 'who', 'was', 'sentenced', 'to', '7', 'years', 'in', '...', 'https', ':', '//t.co/YXH5W53srO', 'I', 'added', 'a', 'video', 'to', 'a', '@', 'YouTube', 'playlist', 'https', ':', '//t.co/CnEyWN40x3', 'Funny', 'Talking', 'of', 'Haryanavi', 'Jat', 'with', 'Canada', 'Immigration', 'Girl', 'Agent', 'Mexicans', 'Can', 'Now', 'Travel', 'Visa-Free', 'To', 'Canada', 'https', ':', '//t.co/Ec3XHORO2s', 'https', ':', '//t.co/RQRr5nebcG', 'L��immigration', 'irr��guli��re', 'au', 'Canada', 'd��cortiqu��e', 'en', '5��questions', 'https', ':', '//t.co/DkpuKyWmaK', '@', 'SweetnessShawnB', 'Hes', 'the', 'POS', 'that', 'ramped', 'up', 'immigration', 'for', 'Canada', ',', 'among', 'other', 'globalist', 'policies', '.', 'Canada', 'lifted', 'visa', 'requirements', 'to', 'Mexico', 'as', 'of', 'Dec', '1', ',', '2016', '.', 'Thoughts', '?', '#', 'visa', '#', 'immigration', '@', 'HuffingtonPost', 'people', 'Keep', 'praising', 'Canada', 'and', 'Canada', 'has', 'way', 'stricter', 'immigration', 'laws', 'then', 'us', 'they', 'willl', 'boot', 'your', 'liberal', 'American', 'ass']\n"
     ]
    }
   ],
   "source": [
    "#read from file\n",
    "fileObjectTwitter2 = open(\"twitter.txt\", \"r\",encoding='utf-8')\n",
    "twitterdata2 = fileObjectTwitter2.read()\n",
    "\n",
    "twitterTokens2 = nltk.word_tokenize(twitterdata2)\n",
    "\n",
    "print(twitterTokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable', '.', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'self-study', 'also', '.', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', '.', '``', 'Good', ':', ')', '<', 'br', '/', '>', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '&', '#', '039', ';', 's', 'better', 'for', 'us', '.', '<', 'br', '/', '>', 'sometimes', 'teaching', 'speed', 'is', 'very', 'high', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Thanks', '!', ':', ')', '<', 'br', '/', '>', '``', 'The', 'lectures', 'are', 'good..but', 'a', 'bit', 'speed.A', 'in', 'class', 'working', 'activity', 'is', 'a', 'must', 'one.So', 'please', 'take', 'another', 'hour', 'in', 'thursdays', 'madame.', \"''\", '<', 'br', '/', '>', 'We', 'can', 'hear', 'your', 'voice', 'clearly', 'and', 'can', 'understand', 'the', 'things', 'you', 'teach', '.', 'Presentation', 'slides', 'also', 'good', 'source', 'to', 'refer', '.', 'lf', 'you', 'can', 'do', 'more', 'example', 'questions', 'within', 'the', 'classroom', 'and', 'it', 'will', 'help', 'us', 'to', 'understand', 'the', 'principles', 'well', '.', '<', 'br', '/', '>', \"''\", 'Lectures', 'was', 'well', 'structured', 'and', 'well', 'organized', '.', 'It', 'was', 'easy', 'to', 'understand', '.', 'Lecture', 'slides', 'and', 'labs', 'were', 'also', 'well', 'organized', '.', 'Lectures', 'were', 'good', '.', 'understandable', '.', 'The', 'lecture', 'slides', 'were', 'well', 'organized', 'and', 'the', 'examples', 'done', 'in', 'the', 'class', 'helped', 'a', 'lot', 'to', 'learn', 'this', 'new', 'language', 'and', 'also', 'the', 'principles', 'of', 'OOP', '.', 'Motivated', 'to', 'well', '.', 'Would', 'have', 'been', 'better', 'if', 'we', 'discussed', 'more', 'about', 'the', 'solutions', 'of', 'coding', 'exercisers', '.', 'I', 'think', 'i', 'learned', 'a', 'lot', 'from', 'the', 'codes', 'you', 'write', 'in', 'the', 'board', '.', 'When', 'i', 'compare', 'my', 'codes', 'with', 'yours', 'i', 'can', 'learn', 'about', 'my', 'mistakes', 'and', 'good', 'coding', 'practices', 'that', 'i', 'should', 'follow', '.', 'There', 'fore', 'i', 'think', 'it', 'would', 'be', 'great', 'if', 'we', 'can', 'discuss', 'more', 'examples', 'in', 'the', 'class', '.', 'madam', 'explained', 'the', 'oop', 'concepts', 'clearly', 'with', 'examples.lectures', 'were', 'interesting.we', 'want', 'more', 'scenario', 'examples', 'and', 'answers', 'with', 'explanations', 'in', 'future', '.', 'I', 'satisfy', 'about', 'first', '7', 'lectures', '.', 'That', 'way', 'of', 'teaching', 'is', 'really', 'good', 'for', 'coming', 'lectures', 'too', '.', 'lectuers', 'are', 'very', 'good', '.', 'take', 'good', 'effort', 'to', 'make', 'undersatand', 'every', 'student', 'in', 'the', 'room', '.', 'very', 'helpfull', '.', 'I', 'was', 'able', 'to', 'obtain', 'a', 'clear', 'picture', 'about', 'OOP', 'and', 'its', 'concepts', '.', '``', 'lecture', 'slides', ',', 'explanations', 'were', 'very', 'clear', '.', '<', 'br', '/', '>', 'it', '&', '#', '039', ';', 's', 'very', 'good', 'to', 'letting', 'ask', 'questions', 'and', 'explain', 'again', 'with', 'suitable', 'examples', '.', '<', 'br', '/', '>', 'sometimes', ',', 'some', 'codes', 'on', 'white', 'board', 'were', 'unclear', 'at', 'the', 'back', '.', '<', 'br', '/', '>', 'overall', 'very', 'good', '!', '!', '!', '<', 'br', '/', '>', \"''\", 'The', 'lectures', 'were', 'good', 'and', 'clear', '.', 'And', 'they', 'weren', '&', '#', '039', ';', 't', 'too', 'fast', '.', 'Writing', 'code', 'was', 'somewhat', 'confusing', 'because', 'I', 'didn', '&', '#', '039', ';', 't', 'know', 'java', 'before', '.', 'Actually', 'teaching', 'is', 'very', 'good', 'and', 'can', 'understand', 'easily', 'the', 'concepts', 'by', 'examples', 'which', 'are', 'given', 'in', 'the', 'class.it', 'will', 'be', 'more', 'helpful', 'if', 'provide', 'solved', 'questions', 'as', 'well', '!', '.', 'thankyou']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fileObjectFeedback = open(\"feedback.txt\", \"r\", encoding='utf-8')\n",
    "feedbackRawData = fileObjectFeedback.read()\n",
    "#feedbackData = BeautifulSoup(feedbackRawData).get_text()\n",
    "feedbackTokens = nltk.word_tokenize(feedbackRawData)\n",
    "\n",
    "print(feedbackTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task', 'learning', ',', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task-invariant', 'features', '.', 'However', ',', 'in', 'most', 'existing', 'approaches', ',', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task-specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', '.', 'In', 'this', 'paper', ',', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'learning', 'framework', ',', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', '.', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', ',', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', '.', 'Besides', ',', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off-the-shelf', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', '.', 'Multi-task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', '.', 'Recently', ',', 'neural-based', 'models', 'for', 'multi-task', 'learning', 'have', 'become', 'very', 'popular', ',', 'ranging', 'from', 'computer', 'vision', '(', 'Misra', 'et', 'al.', ',', '2016', ';', 'Zhang', 'et', 'al.', ',', '2014', ')', 'to', 'natural', 'language', 'processing', '(', 'Collobert', 'andWeston', ',', '2008', ';', 'Luong', 'et', 'al.', ',', '2015', ')', ',', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', '.', 'However', ',', 'most', 'existing', 'work', 'on', 'multi-task', 'learning', '(', 'Liu', 'et', 'al.', ',', '2016c', ',', 'b', ')', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', ',', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', '.', 'As', 'shown', 'in', 'Figure', '1-', '(', 'a', ')', ',', 'the', 'general', 'shared-private', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', ':', 'one', 'is', 'used', 'to', 'store', 'task-dependent', 'features', ',', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', '.', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task-specific', 'features', ',', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', ',', 'suffering', 'from', 'feature', 'redundancy', '.', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', ',', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', ':', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', '.', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', '.', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', '.', 'The', 'word', '�infantile�', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', '.', 'However', ',', 'the', 'general', 'shared-private', 'model', 'could', 'place', 'the', 'task-specific', 'word', '�infantile�', 'in', 'a', 'shared', 'space', ',', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', '.', 'Additionally', ',', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', '.', 'To', 'address', 'this', 'problem', ',', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'framework', ',', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints.Specifically', ',', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence', '.']\n"
     ]
    }
   ],
   "source": [
    "researchfile = open(\"researchpaper.txt\", \"r\", encoding='utf-8')\n",
    "raw = researchfile.read()\n",
    "#data = BeautifulSoup(raw).get_text()\n",
    "researchTokens = nltk.word_tokenize(raw)\n",
    "print(researchTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated word Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\rasika_105127\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.5.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(distance=2)\n",
    "\n",
    "\n",
    "def wordCorrection(tokenSet):\n",
    "    # find those words that may be misspelled\n",
    "    misspelled = spell.unknown(tokenSet)\n",
    "\n",
    "    for word in misspelled:\n",
    "              \n",
    "        print(\"{} -> mostly liked answer : {} \".format(word, spell.correction(word)))\n",
    "        print(\"Mostly Likely Options : {} \".format(spell.candidates(word)))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d'immigration -> mostly liked answer : immigration \n",
      "Mostly Likely Options : {'immigration'} \n",
      "\n",
      "\n",
      "https://t.co/kwz3csvyxm -> mostly liked answer : https://t.co/kwz3csvyxm \n",
      "Mostly Likely Options : {'https://t.co/kwz3csvyxm'} \n",
      "\n",
      "\n",
      "https://t.co/uvruw8er1b -> mostly liked answer : https://t.co/uvruw8er1b \n",
      "Mostly Likely Options : {'https://t.co/uvruw8er1b'} \n",
      "\n",
      "\n",
      "https://t.co/j77devjoix -> mostly liked answer : https://t.co/j77devjoix \n",
      "Mostly Likely Options : {'https://t.co/j77devjoix'} \n",
      "\n",
      "\n",
      "@youtube -> mostly liked answer : @youtube \n",
      "Mostly Likely Options : {'@youtube'} \n",
      "\n",
      "\n",
      "#website -> mostly liked answer : #website \n",
      "Mostly Likely Options : {'#website'} \n",
      "\n",
      "\n",
      "262m -> mostly liked answer : m62 \n",
      "Mostly Likely Options : {'m62'} \n",
      "\n",
      "\n",
      "https://t.co/5lievho7a4 -> mostly liked answer : https://t.co/5lievho7a4 \n",
      "Mostly Likely Options : {'https://t.co/5lievho7a4'} \n",
      "\n",
      "\n",
      "https://t.co/kxdfmgtzzn -> mostly liked answer : https://t.co/kxdfmgtzzn \n",
      "Mostly Likely Options : {'https://t.co/kxdfmgtzzn'} \n",
      "\n",
      "\n",
      "besoin -> mostly liked answer : begin \n",
      "Mostly Likely Options : {'bestir', 'besom', 'bedwin', 'resin', 'basin', 'lesion', 'beso', 'beeson', 'bertin', 'bastin', 'bergin', 'boson', 'berlin', 'busoni', 'dessin', 'bedouin', 'benin', 'beshir', 'bessie', 'meson', 'bustin', 'bemoan', 'beron', 'eosin', 'espin', 'bison', 'heroin', 'benoit', 'rejoin', 'begin', 'breslin', 'bevin', 'belbin', 'eoin', 'benson'} \n",
      "\n",
      "\n",
      "@shawhelp -> mostly liked answer : @shawhelp \n",
      "Mostly Likely Options : {'@shawhelp'} \n",
      "\n",
      "\n",
      "vaste -> mostly liked answer : waste \n",
      "Mostly Likely Options : {'paste', 'haste', 'caste', 'vast', 'baste', 'vase', 'waste', 'vaster', 'taste'} \n",
      "\n",
      "\n",
      "https://t.co/lsg7c3vle9 -> mostly liked answer : https://t.co/lsg7c3vle9 \n",
      "Mostly Likely Options : {'https://t.co/lsg7c3vle9'} \n",
      "\n",
      "\n",
      "fear-mongering -> mostly liked answer : fear-mongering \n",
      "Mostly Likely Options : {'fear-mongering'} \n",
      "\n",
      "\n",
      "#visa -> mostly liked answer : visa \n",
      "Mostly Likely Options : {'visa'} \n",
      "\n",
      "\n",
      "#usa -> mostly liked answer : usa \n",
      "Mostly Likely Options : {'cusa', 'susa', 'musa', 'nusa', 'usa'} \n",
      "\n",
      "\n",
      "canada's -> mostly liked answer : canada \n",
      "Mostly Likely Options : {'canadians', 'canada'} \n",
      "\n",
      "\n",
      "https://t.co/x2ifo0exi2 -> mostly liked answer : https://t.co/x2ifo0exi2 \n",
      "Mostly Likely Options : {'https://t.co/x2ifo0exi2'} \n",
      "\n",
      "\n",
      "xkofy -> mostly liked answer : goofy \n",
      "Mostly Likely Options : {'goofy', 'kafy', 'xsoft'} \n",
      "\n",
      "\n",
      "ays -> mostly liked answer : as \n",
      "Mostly Likely Options : {'ars', 'avs', 'pays', 'als', 'lys', 'says', 'aps', \"a's\", 'ayr', 'rays', 'ayes', 'a.s', 'ways', 'ahs', 'bays', 'aes', 'gays', 'ams', 'ans', 'kays', 'cys', 'hays', 'abs', 'afs', 'acs', 'as', 'ys', 'yas', 'ads', 'days', 'mays', 'aye', 'jays', 'ass', 'a/s', 'sys', 'aos', 'lays', 'ats', 'ay', 'aas', 'ais', 'aus'} \n",
      "\n",
      "\n",
      "https://t.co/ec3xhoro2s -> mostly liked answer : https://t.co/ec3xhoro2s \n",
      "Mostly Likely Options : {'https://t.co/ec3xhoro2s'} \n",
      "\n",
      "\n",
      "it's -> mostly liked answer : its \n",
      "Mostly Likely Options : {\"i's\", \"it'\", \"iv's\", \"t's\", \"ix's\", 'its', \"ii's\"} \n",
      "\n",
      "\n",
      "#fasttraffic -> mostly liked answer : #fasttraffic \n",
      "Mostly Likely Options : {'#fasttraffic'} \n",
      "\n",
      "\n",
      "#statistics -> mostly liked answer : statistics \n",
      "Mostly Likely Options : {'statistics'} \n",
      "\n",
      "\n",
      "https://t.co/ihpvhw2bag -> mostly liked answer : https://t.co/ihpvhw2bag \n",
      "Mostly Likely Options : {'https://t.co/ihpvhw2bag'} \n",
      "\n",
      "\n",
      "jat -> mostly liked answer : at \n",
      "Mostly Likely Options : {'kat', 'jot', 'jet', 'wat', 'jct', 'fat', 'yat', 'tat', 'jay', 'jaz', 'jag', 'jaq', 'jut', 'jam', 'oat', 'jas', 'sat', 'jaw', 'jap', 'at', 'gat', 'nat', 'jac', 'jt', 'jal', 'bat', 'mat', 'cat', 'jit', 'ja', 'dat', 'hat', 'jar', 'jwt', 'lat', 'vat', 'pat', 'jai', 'rat', 'jaa', 'eat', 'jab', 'jan', 'jah'} \n",
      "\n",
      "\n",
      "https://t.co/yagwmz8ecp -> mostly liked answer : https://t.co/yagwmz8ecp \n",
      "Mostly Likely Options : {'https://t.co/yagwmz8ecp'} \n",
      "\n",
      "\n",
      "#immigration -> mostly liked answer : immigration \n",
      "Mostly Likely Options : {'immigration'} \n",
      "\n",
      "\n",
      "#cdnpoli -> mostly liked answer : #cdnpoli \n",
      "Mostly Likely Options : {'#cdnpoli'} \n",
      "\n",
      "\n",
      "=p -> mostly liked answer : up \n",
      "Mostly Likely Options : {'qp', 'ep', 'up', 'p', 'vp', 'mp', 'np', 'ap', 'p=', 'ip', 'cp', 'gp', 'fp', 'sp', 'pp', 'op', 'rp', 'kp', 'hp', 'bp', 'tp', 'wp', 'xp', 'dp', 'jp', 'zp', 'lp'} \n",
      "\n",
      "\n",
      "globalist -> mostly liked answer : loyalist \n",
      "Mostly Likely Options : {'localist', 'loyalist'} \n",
      "\n",
      "\n",
      "guli -> mostly liked answer : gulf \n",
      "Mostly Likely Options : {'guil', 'gull', 'gui', 'guls', 'gul', 'gulp', 'gulf', 'gulu', 'quli', 'uli'} \n",
      "\n",
      "\n",
      "https://t.co/m5ckgyvv8f -> mostly liked answer : https://t.co/m5ckgyvv8f \n",
      "Mostly Likely Options : {'https://t.co/m5ckgyvv8f'} \n",
      "\n",
      "\n",
      "https://t.co/9i72frhtij -> mostly liked answer : https://t.co/9i72frhtij \n",
      "Mostly Likely Options : {'https://t.co/9i72frhtij'} \n",
      "\n",
      "\n",
      "https://t.co/becgusy2i6 -> mostly liked answer : https://t.co/becgusy2i6 \n",
      "Mostly Likely Options : {'https://t.co/becgusy2i6'} \n",
      "\n",
      "\n",
      "that's -> mostly liked answer : thats \n",
      "Mostly Likely Options : {\"that'\", 'thats'} \n",
      "\n",
      "\n",
      "haryanavi -> mostly liked answer : haryana \n",
      "Mostly Likely Options : {'haryana'} \n",
      "\n",
      "\n",
      "https://t.co/zozose1cqq -> mostly liked answer : https://t.co/zozose1cqq \n",
      "Mostly Likely Options : {'https://t.co/zozose1cqq'} \n",
      "\n",
      "\n",
      "#racism -> mostly liked answer : racism \n",
      "Mostly Likely Options : {'racism'} \n",
      "\n",
      "\n",
      "https://t.co/4k84ee8y63 -> mostly liked answer : https://t.co/4k84ee8y63 \n",
      "Mostly Likely Options : {'https://t.co/4k84ee8y63'} \n",
      "\n",
      "\n",
      "deyfy -> mostly liked answer : defy \n",
      "Mostly Likely Options : {'defy'} \n",
      "\n",
      "\n",
      "#lpc -> mostly liked answer : pc \n",
      "Mostly Likely Options : {'npc', 'abpc', 'jlp', 'plec', 'ppc', 'pc', 'lps', 'lhc', 'lc', 'wpc', 'lrc', 'lpi', 'llp', 'lp', 'elec', 'tlc', 'alp', 'flic', 'rdpc', 'plac', 'appc', 'bloc', 'bpc', 'kpc', 'dlp', 'hplc', 'luc', 'ulcc', 'rlfc', 'flec', 'mepc', 'hpc', 'gpc', 'mpc', 'plp', 'lac', 'ltc', 'apc', 'ilp', 'lpf', 'glp', 'fpc', 'plc', 'lp*', 'ulph', 'lpo', 'lgc', 'alec', 'clp', 'lbc', 'irpc', 'bldc', 'mlc', 'mlp', 'ldc', 'l-c', 'flp', 'spc', 'lcc', 'dlc', 'vlps', 'flnc', 'lpg', 'glc', 'lpk', 'nlp', 'cpc', 'rpc', 'epc', 'clps', 'alps', 'dpc', 'lmc', 'ilps', 'loc', 'slp', 'clrc', 'tpc', 'ipc', 'lec'} \n",
      "\n",
      "\n",
      "https://t.co/s3hu1okkig -> mostly liked answer : https://t.co/s3hu1okkig \n",
      "Mostly Likely Options : {'https://t.co/s3hu1okkig'} \n",
      "\n",
      "\n",
      "https://t.co/uibsezoqas -> mostly liked answer : https://t.co/uibsezoqas \n",
      "Mostly Likely Options : {'https://t.co/uibsezoqas'} \n",
      "\n",
      "\n",
      "https://t.co/zrlj26jnkc -> mostly liked answer : https://t.co/zrlj26jnkc \n",
      "Mostly Likely Options : {'https://t.co/zrlj26jnkc'} \n",
      "\n",
      "\n",
      "website -> mostly liked answer : webster \n",
      "Mostly Likely Options : {'resite', 'webster'} \n",
      "\n",
      "\n",
      "#immigrants -> mostly liked answer : immigrants \n",
      "Mostly Likely Options : {'immigrants'} \n",
      "\n",
      "\n",
      "@canadidly -> mostly liked answer : candidly \n",
      "Mostly Likely Options : {'candidly'} \n",
      "\n",
      "\n",
      "... -> mostly liked answer : p. \n",
      "Mostly Likely Options : {'f.a.', 'c.n.', 'l.r.', 's.i', 'q.t.', 'm.i.', 'f.i.', 'f.w.', 'p.6', 'cf.', 'c.s.', 'd.f.', 'p.p.', 'e.g.', 'r.h.', 'l.s.', 'lt.', 'n.d.', 'a.2', 'f.c.', 'b.l.', 'q.v', 'll.', 't.h.', 'f.r.', 'sp.', 'j.l.', 'q.c', 'u.s.', 'ct.', 'p.t.', 't.a.', 'e.h', 's.j.', 'ad.', 'ch.', 'i.', 'd.r.', 'l.p.', 'n.i.', 't.j.', 'n.y.', 'b.r.', 'k.', 'g.h.', 'r.o.', 'a.h', 'x.', 'h.e', 'm.', 'i.1', 'b.c.', 'op.', 'h.m', 'h.q', 'a.c.', 'p.5', 'k.b', 't.c.', 'oe.', 'h.c.', 'p.7', 'y.', 'a.p.', 'p.r.', 'c.e.', 'g.j.', 'h.b.', 'w.c.', 't.s.', 'a.e.', 'j.r.', 'e.b.', 'md.', 'l.t.', 'j.', 'c.z.', 'w.i.', 'e.r', 'g.b.', 'c.p.', 'v.e.', 'tn.', 'cm.', 'a.f.', 'c.l.', 'e.t.', 'v.p.', 'l.b.', 'b.p.', 'ca.', 's.s.', 'j.n.', 'r.w.', 'pt.', 'ps.', 'i.d', 'hm.', 'p.9', 'ms.', 'e.j.', 'oz.', 'o.p.', 'f.g.', 'a.1', 'a/.', 'l.j.', 'k.b.', 'c.j.', 'c.b.', 's.6', 'n.d', 'ff.', 'o.g.', 's.5', 'm.p', 'h.d.', 'rd.', 's.2', 'p.w.', 'a.s.', 'a.b', 'b.s.', 'g.e.', 'w.e', 'd.c.', 'm.d.', 'p.2', 'm.g.', 'r.e', 'fi.', 'w.t.', 'i.a.', 'ie.', 's.d.', 'n.w.', 'b.1', 'u.s', 'e.f.', 'q.b.', 'l.', 'd.e', 'f.l.', 'a.d.', 'r.d.', 'm.y', 'u.k.', 'e.c.', 'p.c.', 'a.7', 'g.a.', 'h.g.', 'p.8', 'c.o.', 'p.m', 'p.1', 'j.c.', 'n.j.', 'g.o', 'w.b.', 'pp.', 'c.2', 'a.l', 'u.n.', 'dr.', 'h.', 'c.d.', 'n.', 'cu.', 'd.a', 'a.r.', 'pg.', 'fa.', 'a.', 'l.m.', 'c.f.', 'a.s', 'vi.', 'e.v.', 'c.', 'e.', 'j.g.', 'br.', 'l.a.', 'a.3', 'f.j.', 'p.e', 'c.r.', 'mr.', 'z.', 'km.', 'is.', 'd.', 'r.l.', 'n.l.', 'ex.', 's.i.', 'j.a.', 'h.q.', 'st.', 's.g.', 'c.5', 'p.l.', 'e.g', 'w.c', 'f.', 'r.m.', 'w.g.', 's.o', 'g.m.', 'g.l.', 's.p.', 's.e.', 'v.i.', 'r.a.', 'o.', 'w.h.', 'jr.', 'ss.', 'j.p', 'c.t.', 's.b.', 'a.t', 'j.j.', 'j.s.', 'i.k.', 'l.g.', 'e.w.', 't.r.', 'kg.', 's.w.', 'j.p.', 'n.b.', 'r.s.', 'a.w', 'v.r.', 'am.', 'p.s.', 'm.r.', 'i.v.', 's.4', 'a.n', 'pc.', 'c.v.', 'w.d.', 'w.j.', 'u.', 'r.j.', 'p.o', 'd.h.', 'r.r.', 'f.e.', 'g.k.', 'r.k.', 'g.s.', 't.e.', 'j.w.', 'c.h.', 'p.a.', 'l.l.', 'h.j.', 'sc.', 'ed.', 'j.t.', 'fl.', 'n.h.', 'i.e', 'v.c', 'fr.', 'i.e.', 'j.f.', 'p.4', 'r.', 'sr.', 'me.', 'ft.', 'g.', 's.r.', 'j.m.', 'co.', 'q.v.', 'a.r', 'uk.', 'p.g.', 'b.2', 'h.t.', 'e.s.', 's.a.', 'p.o.', 'r.b.', 'p.c', 'w.', 'o.r', 'k.g.', 'iv.', 'i.m.', 'o.s.', 's.t', 'pm.', 'ii.', 's.m.', 'h.a', 'c.g.', 'at.', 'cp.', 'd.m.', 'eg.', 'i.t', 'jj.', 't.', 'g.d.', 'c.c.', 'us.', 'w.r.', 'c.a', 'g.f.', 'c.w.', 'd.g.', 'a.4', 's.', 'b.t.', 'q.', 'no.', 'n.s.', 'm.p.', 'm.s.', 'pl.', 'av.', 'a.j.', 'w.s.', 'w.a.', 'r.n.', 'v.i', 's.a', 'j.h.', 'lb.', 'p.h.', 'h.h.', 'e.a.', 'r.f.', 'm.e.', 'p.a', 'vs.', 'b.', 'b.a.', 's.1', 't.4', 'm.a.', 'v.4', 'h.s.', 'e.p.', 'v.', 'n.a.', 'h.o', 't.v.', 'hr.', 'p.j.', 'eq.', 'd.j', 'c.a.', 'a.g.', 'p.3', 'o.t.', 'w.e.', 'w.m.', 'g.w.', 'q.c.', 'n.c.', 'e.r.', 'n.e.', 'p.b.', 'b.d.', 's.9', 'a.a.', 'd.d.', 'l.c.', 'm.o.', 'f.i', 'sq.', 's.c.', 'p.', 'a.m.', 'm.d', 'a.k.', 'b.b.', 'g.c.', 'dw.', 'cl.', 'r.c.', 'h.m.', 'e.m.', 's.3', 'o.k.', 'd.v.', 'i.c.', 'a.b.', 'a.m', 'p.m.', 'p.d.', 'd.s.', 'g.p.', 'mp.', 'r.t.', 'j.e.', 'a.i.', 'u.k', 'j.d.', 'r.i.', 'j.r', 'p.e.', 'h.p.', 'v.c.', 'm.c.', 'al.', 'it.', 'in.', 'q.t', 'm.b.', 's.8', 'j.b.', 'h.l.'} \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://t.co/f4uto5a7zf -> mostly liked answer : https://t.co/f4uto5a7zf \n",
      "Mostly Likely Options : {'https://t.co/f4uto5a7zf'} \n",
      "\n",
      "\n",
      "#canada -> mostly liked answer : canada \n",
      "Mostly Likely Options : {'canada'} \n",
      "\n",
      "\n",
      "https://t.co/cyi867pzrv -> mostly liked answer : https://t.co/cyi867pzrv \n",
      "Mostly Likely Options : {'https://t.co/cyi867pzrv'} \n",
      "\n",
      "\n",
      "https://t.co/nefw30mraa -> mostly liked answer : https://t.co/nefw30mraa \n",
      "Mostly Likely Options : {'https://t.co/nefw30mraa'} \n",
      "\n",
      "\n",
      "family's -> mostly liked answer : family \n",
      "Mostly Likely Options : {'families', 'family', 'familes'} \n",
      "\n",
      "\n",
      "https://t.co/wtqk4qdiki -> mostly liked answer : https://t.co/wtqk4qdiki \n",
      "Mostly Likely Options : {'https://t.co/wtqk4qdiki'} \n",
      "\n",
      "\n",
      "uvre -> mostly liked answer : ure \n",
      "Mostly Likely Options : {'ure'} \n",
      "\n",
      "\n",
      "abhinav -> mostly liked answer : abhinav \n",
      "Mostly Likely Options : {'abhinav'} \n",
      "\n",
      "\n",
      "monsef -> mostly liked answer : money \n",
      "Mostly Likely Options : {'mosel', 'monster', 'monie', 'moness', 'monies', 'morsel', 'mosses', 'minse', 'mons', 'mouse', 'mouses', 'moser', 'mounsey', 'mansel', 'mosse', 'menses', 'jozsef', 'yosef', 'monnet', 'mouser', 'monied', 'monkey', 'monzer', 'onset', 'moses', 'monet', 'moises', 'montes', 'mondeo', 'moose', 'moise', 'monde', 'manse', 'money', 'manser', 'monte', 'mousey', 'josef', 'mons.', 'mosey', 'morse'} \n",
      "\n",
      "\n",
      "#hatecrime -> mostly liked answer : #hatecrime \n",
      "Mostly Likely Options : {'#hatecrime'} \n",
      "\n",
      "\n",
      "https://t.co/rqrr5nebcg -> mostly liked answer : https://t.co/rqrr5nebcg \n",
      "Mostly Likely Options : {'https://t.co/rqrr5nebcg'} \n",
      "\n",
      "\n",
      "cortiqu -> mostly liked answer : cortina \n",
      "Mostly Likely Options : {'cortina', 'corti'} \n",
      "\n",
      "\n",
      "https://t.co/cneywn40x3 -> mostly liked answer : https://t.co/cneywn40x3 \n",
      "Mostly Likely Options : {'https://t.co/cneywn40x3'} \n",
      "\n",
      "\n",
      "#jamaican -> mostly liked answer : jamaican \n",
      "Mostly Likely Options : {'jamaican'} \n",
      "\n",
      "\n",
      "https://t.co/xxdeig7dbu -> mostly liked answer : https://t.co/xxdeig7dbu \n",
      "Mostly Likely Options : {'https://t.co/xxdeig7dbu'} \n",
      "\n",
      "\n",
      "#integration -> mostly liked answer : integration \n",
      "Mostly Likely Options : {'integration'} \n",
      "\n",
      "\n",
      "willl -> mostly liked answer : will \n",
      "Mostly Likely Options : {'wills', 'willy', 'willi', 'will', 'wille', 'willa'} \n",
      "\n",
      "\n",
      "@theeconomist -> mostly liked answer : @theeconomist \n",
      "Mostly Likely Options : {'@theeconomist'} \n",
      "\n",
      "\n",
      "l'immigration -> mostly liked answer : immigration \n",
      "Mostly Likely Options : {'immigration'} \n",
      "\n",
      "\n",
      "#traffic -> mostly liked answer : traffic \n",
      "Mostly Likely Options : {'traffic'} \n",
      "\n",
      "\n",
      "@ladymadonna___ -> mostly liked answer : @ladymadonna___ \n",
      "Mostly Likely Options : {'@ladymadonna___'} \n",
      "\n",
      "\n",
      "https://t.co/dkpukywmak -> mostly liked answer : https://t.co/dkpukywmak \n",
      "Mostly Likely Options : {'https://t.co/dkpukywmak'} \n",
      "\n",
      "\n",
      "@sweetnessshawnb -> mostly liked answer : @sweetnessshawnb \n",
      "Mostly Likely Options : {'@sweetnessshawnb'} \n",
      "\n",
      "\n",
      "@huffingtonpost -> mostly liked answer : @huffingtonpost \n",
      "Mostly Likely Options : {'@huffingtonpost'} \n",
      "\n",
      "\n",
      "i've -> mostly liked answer : ive \n",
      "Mostly Likely Options : {'ive'} \n",
      "\n",
      "\n",
      "#sitetraffic -> mostly liked answer : #sitetraffic \n",
      "Mostly Likely Options : {'#sitetraffic'} \n",
      "\n",
      "\n",
      "rigoureusement -> mostly liked answer : rigoureusement \n",
      "Mostly Likely Options : {'rigoureusement'} \n",
      "\n",
      "\n",
      "#onthisday -> mostly liked answer : #onthisday \n",
      "Mostly Likely Options : {'#onthisday'} \n",
      "\n",
      "\n",
      "https://t.co/99myliuoes -> mostly liked answer : https://t.co/99myliuoes \n",
      "Mostly Likely Options : {'https://t.co/99myliuoes'} \n",
      "\n",
      "\n",
      "#greencard -> mostly liked answer : #greencard \n",
      "Mostly Likely Options : {'#greencard'} \n",
      "\n",
      "\n",
      "� -> mostly liked answer : a \n",
      "Mostly Likely Options : {'w', 't', 'y', 'z', 'x', 'g', 'o', 'p', 'm', 'h', 'd', 'l', 'b', 'f', 'a', 'e', 's', 'v', 'n', 'c', 'r', 'j', 'k', 'i', 'q', 'u'} \n",
      "\n",
      "\n",
      "https://t.co/0c5obfmxlg -> mostly liked answer : https://t.co/0c5obfmxlg \n",
      "Mostly Likely Options : {'https://t.co/0c5obfmxlg'} \n",
      "\n",
      "\n",
      "https://t.co/yxh5w53sro -> mostly liked answer : https://t.co/yxh5w53sro \n",
      "Mostly Likely Options : {'https://t.co/yxh5w53sro'} \n",
      "\n",
      "\n",
      "https://t.co/vcmfygadr5 -> mostly liked answer : https://t.co/vcmfygadr5 \n",
      "Mostly Likely Options : {'https://t.co/vcmfygadr5'} \n",
      "\n",
      "\n",
      "#cpcldr -> mostly liked answer : #cpcldr \n",
      "Mostly Likely Options : {'#cpcldr'} \n",
      "\n",
      "\n",
      "campagne -> mostly liked answer : champagne \n",
      "Mostly Likely Options : {'champagne', 'campagna'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCorrection(twitterTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "madame. -> mostly liked answer : madame \n",
      "Mostly Likely Options : {'madame'} \n",
      "\n",
      "\n",
      "weren -> mostly liked answer : were \n",
      "Mostly Likely Options : {'beren', 'keren', 'wren', 'were'} \n",
      "\n",
      "\n",
      "`` -> mostly liked answer : of \n",
      "Mostly Likely Options : {'z', 'hh', 'un', 'dj', 'h3', 'll', 'p', 'ps', 'ee', 'sv', 'vc', 'yr', 'dx', 'of', 'eg', 'rc', 'ft', 'es', 'n1', 'a3', 'm0', 'y-', 'yd', 'at', 'ch', 'kc', 'ni', 'sp', 'cy', 'y2', 'dv', 'hl', 'sr', 'go', 'mm', 'ml', 'su', 'lc', 'sb', 'zz', 'r=', 'wa', 'ws', 'c', 'q7', 'l-', 'f1', 'pk', 'yo', 'ca', 'oa', 'um', 'lp', 'h1', 'm3', 'hr', 'my', 'gs', 'ai', 'ie', 'vf', 'vh', 'ox', 'ku', 'og', 'wd', \"p'\", 'np', 'q9', 'ar', 'bs', 'b9', 'va', 'uv', 'rw', 'i.', 'bm', 'cu', 'rh', 'ss', 'vt', 'z+', 'b6', 'gc', 'pu', 'jt', 're', 'os', 't4', 'h5', 'bu', 'l5', 'b+', 'k.', 'c7', 'm9', 'x.', 'rb', 'cj', 'v1', 'kr', 'nt', 'ib', 'bf', 'sk', 'c-', 'r-', 'm.', 'cw', 'cd', 'ea', 'em', 'k2', 'y', 'ff', 'in', 'sz', 'pv', 'n=', 'mq', 'lx', 'rl', 'je', 'la', 'xv', 'v6', 'hq', 'kk', 'ks', 'r3', 'y.', 'uk', 'pf', 'lz', 'ho', 'ih', 'ax', 'ds', 'a6', 'rf', 'a*', 'oc', 'g-', 'td', 'o%', 'we', 'a1', 'pq', 't-', 'gx', 'p%', \"r'\", 'vd', 'sg', 's9', 'il', 'px', 'mn', 'oh', 'h7', 'rx', 'pw', 'to', 'ep', 'lt', 'j.', 'dt', 'tu', 'ur', 'sq', 'gg', 'x1', 'iq', 'jh', 'xm', 'nd', 'zx', 've', 'nv', 'af', 'kl', 'f', 'fi', 'gy', 'ld', \"q'\", 'ms', 'd-', 't=', 'oy', 'kp', 'e1', 'f8', 'a-', 'ry', 'xa', 'a2', 'c2', 'tw', 'mj', 'zu', 'x-', 'hg', 'a9', 'm-', 'gn', 'z%', 'nw', 'k', 'cm', 'q', 'nb', 'ay', 'r4', 'ek', 'jc', 'wc', 'ly', 'b3', 'ig', 'w4', 'e4', 'ad', 'h2', 'b5', 'r0', 'r/', 'oo', 'iz', 'et', 'yn', 'f7', 'yz', 'pc', 'kx', 'b', 'dd', 'vj', 's', 'm1', 'op', 'qa', 'as', 'c1', 'n', 'ro', 'kf', 'ia', 'ob', 'vo', 'eq', 'mt', 'ls', 'aw', 'p-', 'fv', 'r6', 'kh', 'jo', 'w2', 'sh', 'e2', 'mv', 'e7', 'u1', 'b8', 'rg', 'fx', \"n'\", 'xs', 'g', \"y'\", 'bl', 'cv', 'pj', 'rv', 'ny', 'nu', 'ec', 'n5', 'g/', 'am', 'd6', 'd9', 'p4', 'se', 'eu', 'sj', 'rp', 'iv', 'bn', 'bh', 'rs', 'wg', 'ub', 'fm', 'ce', 'fw', \"j'\", 'xz', 'w1', 'l*', 'fl', 'bk', 'l.', 'ma', 'pd', 'st', 'ol', 'lv', 'm7', 'g1', 'fc', 'gh', 'ah', 'v2', 'w6', 'tb', \"i'\", 'gd', 'w3', 'd2', 'ru', 'ck', 'm', 'wt', 'qd', \"m'\", 'ip', 'ew', 'nr', 'ln', 'a7', 'bd', 'n3', 'mh', 'gu', 'ge', 'io', 'p2', 'hs', 's7', 'ns', 'oi', 't3', 'ud', 'qu', 'wm', 'sa', 'mz', 'z3', 'qc', 'yc', 'hw', 'l2', 'xc', 'jp', 'r', 'pt', \"s'\", 'g4', 'zp', 'wh', 'fg', 'i', 'gq', 'xt', 'wn', 'lh', 'h.', 'tt', 't', 'lm', 'lf', 'ko', 'cl', 'n.', 'e5', 'vp', 'a.', 'tk', 'l3', 'v9', 'yu', 'f6', 'tc', \"a'\", 'cp', 'c.', 'e.', 'mg', 'tx', 'n+', 'ev', 'i1', 'do', 'e', 'bt', 'qb', 'y1', 'uf', 'hj', 'z.', 'ja', 'co', 'by', 'd.', 'hd', 'cc', 'xp', 'tn', 'p6', 'g2', 'nj', 'wu', 'sl', 'x2', 'lj', 'lu', \"c'\", 'b4', 'pb', 'jl', 'kv', 'jn', 'c/', 'dn', 'g7', 'sy', 'ic', 'o', 'g8', 'hu', 'v8', 'is', 'fk', 'dl', 'o2', 'f.', 't9', 'r2', 'kg', 't1', 'ae', 's4', 'ct', 'lo', 'e8', 'it', 'm5', 'ke', 'fa', 'h6', 'p5', 'ci', 'hn', 'mi', 'o.', 'uh', 'oz', 'tp', 'm8', 's6', 'hk', 'qw', 'xu', 'gr', 'ky', 'th', 'd4', \"e'\", 'bj', 'g9', 'ki', 'df', 'ra', 'w8', 'l1', 'q4', 'da', 'ty', 'jf', 'el', 'vs', 'w7', 'w', 'rt', 'n4', 'he', 'g5', 'k4', 'd5', 'jd', 'ba', 'ou', 'tr', 'bo', 'mb', 'ww', 'u.', 'vv', 'en', 'ok', 'qt', 'mw', 'ux', 'f2', 'sm', 'bv', 'dg', 'k3', 'tm', 'hp', 'du', 'ao', 'rr', 'r9', 'fr', 'gm', 'ab', 'ye', 'dy', 'vg', 'dp', 'qv', 'l7', 'wr', 'tj', 'g6', 'kb', 'j', 'cx', 'rn', 'ed', 'lg', 'r5', 'hb', 'q2', 'cn', 'uu', 'dc', 'nm', 'sn', 'fu', 'vr', 'pe', 'qi', 'h', 'f4', 'p1', 'ts', 'm2', 'x%', 'h4', 'nl', 'fs', 'ri', 'a4', 'xi', 'z2', 'gp', 'r.', 'gt', 'ak', 'kn', 'g.', 'fn', 'l6', 'qm', 'or', 'e6', 'an', 'na', 'fy', 'ag', 'br', 'yy', 'dk', 'vi', 'eh', 'cg', 'w.', 'u', 'ir', 't7', 'ze', 'bb', 'hf', 'xy', 'gv', 'kt', \"t'\", 'iu', 'qs', 'f3', 'ui', 'mp', 'g3', 'l', 'd/', 'vl', 'w ', 'mc', 'v4', 'n6', 'sf', 'jj', 'rd', 'zn', 'ij', 'ya', 'kd', 'di', 't.', 'ey', 'ov', 'ng', 'sw', 'al', 'c5', 'mo', 'im', 'dq', 's.', 'c8', 'rj', 'q.', 'sc', 'mu', 'nc', 'rm', 's2', 'pr', 'up', 'jb', 'c6', 'fo', 'yi', 'lr', 'ha', 'e-', 's8', 'a8', 'ap', 'f9', 'nf', 't6', 'vw', 'vu', 's3', 'ac', 'p7', 'u6', 'ef', 'r7', 'li', 'mx', 'p8', 'tl', 'bp', 'h8', 'cr', 'b1', 'rk', 'l9', 'b-', 'k1', 'ta', 'w9', 'd3', 'av', 'tg', 'n2', 'ju', 'md', 'db', 't2', 'ow', 'aj', 'hi', 'd8', 'nh', 'ne', 'bq', 'b7', 'q6', 'qp', 'eo', 'wi', 'h*', 'r*', 'ka', 'p3', 'bw', 'x3', 'be', 'ga', 'gi', 'yt', 'b.', 'vb', 'bi', 'dr', 't5', 'km', 'kj', 'a+', 'le', 'od', 'cs', 'dm', 'a', 'bg', 'cf', 'i-', 'bc', 'm6', 'us', 'v.', 'wo', \"o'\", 'a5', 'si', 'wp', 'i/', 'mf', 'q3', 'wb', 'z1', 's0', 'bx', 'u2', 'no', 'd7', 'l4', 'y ', 'ex', 'aa', 'c3', 'dw', 'te', 'd1', 'x', 'x4', 'kw', 'xj', 'ht', 'tv', 'fe', 'po', 'jm', 'uc', 'xd', 'fp', 'om', 'r1', 'l+', 'if', 'ua', 'p9', 'l8', 'mr', 'p.', 'de', 'm4', 'q8', 'xl', 'c+', 'n7', 'q1', 'jr', 'gb', 'ut', 'dz', 'b2', 'pn', 'nz', 'ji', 'hc', 'zo', 's1', 'au', 'f+', 'pa', 'pi', 'eb', 'pg', 'w5', 'ph', 'gl', 'js', 's5', 'ot', 'xx', 'xr', 'id', 'sx', 'd', 'p=', 'cb', 'dh', 'oe', 'lb', 'ix', 'vm', 'gw', 'nk', 'hm', 'c4', 'ii', 'hz', 'v', 'lw', 'ei', 'pp', 'e3', 'f5', 'pl', 'er', \"f'\", 'fb', 'ys', \"d'\", 'qr', 'r8', 'so', 'nn', 'pm', 'c9', 'sd', 'mk', 'on', 'me', 'n-', 'ti'} \n",
      "\n",
      "\n",
      "helpfull -> mostly liked answer : helpful \n",
      "Mostly Likely Options : {'helpful', 'helpfully'} \n",
      "\n",
      "\n",
      "undersatand -> mostly liked answer : understand \n",
      "Mostly Likely Options : {'understand'} \n",
      "\n",
      "\n",
      "examples.lectures -> mostly liked answer : examples.lectures \n",
      "Mostly Likely Options : {'examples.lectures'} \n",
      "\n",
      "\n",
      "interesting.we -> mostly liked answer : interesting.we \n",
      "Mostly Likely Options : {'interesting.we'} \n",
      "\n",
      "\n",
      "class.it -> mostly liked answer : classic \n",
      "Mostly Likely Options : {'classic'} \n",
      "\n",
      "\n",
      "lectuers -> mostly liked answer : lectures \n",
      "Mostly Likely Options : {'lectures', 'lecturers'} \n",
      "\n",
      "\n",
      "exercisers -> mostly liked answer : exercises \n",
      "Mostly Likely Options : {'exercises'} \n",
      "\n",
      "\n",
      "speed.a -> mostly liked answer : speed \n",
      "Mostly Likely Options : {'speedway', 'speedo', 'speeded', 'speedie', 'speeder', 'speedy', 'speeds', 'speed'} \n",
      "\n",
      "\n",
      "one.so -> mostly liked answer : ones \n",
      "Mostly Likely Options : {\"one's\", 'ones', 'oneish'} \n",
      "\n",
      "\n",
      "good..but -> mostly liked answer : good..but \n",
      "Mostly Likely Options : {'good..but'} \n",
      "\n",
      "\n",
      "'' -> mostly liked answer : d' \n",
      "Mostly Likely Options : {\"m'\", \"s'\", \"e'\", \"r'\", \"a'\", \"n'\", \"o'\", \"i'\", \"y'\", \"t'\", \"f'\", \"c'\", \"q'\", \"d'\", \"p'\", \"j'\"} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCorrection(feedbackTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task-specific -> mostly liked answer : task-specific \n",
      "Mostly Likely Options : {'task-specific'} \n",
      "\n",
      "\n",
      "orthogonality -> mostly liked answer : orthogonality \n",
      "Mostly Likely Options : {'orthogonality'} \n",
      "\n",
      "\n",
      "1- -> mostly liked answer : i- \n",
      "Mostly Likely Options : {'l-', 'x-', 'p-', 'a-', 'g-', 'm-', 'y-', 'c-', 'r-', 'b-', 'i-', 'd-', 't-', 'n-', 'e-'} \n",
      "\n",
      "\n",
      "constraints.specifically -> mostly liked answer : constraints.specifically \n",
      "Mostly Likely Options : {'constraints.specifically'} \n",
      "\n",
      "\n",
      "andweston -> mostly liked answer : anderton \n",
      "Mostly Likely Options : {'anderton'} \n",
      "\n",
      "\n",
      "2016c -> mostly liked answer : 2016c \n",
      "Mostly Likely Options : {'2016c'} \n",
      "\n",
      "\n",
      "collobert -> mostly liked answer : colbert \n",
      "Mostly Likely Options : {'colbert'} \n",
      "\n",
      "\n",
      "task-invariant -> mostly liked answer : task-invariant \n",
      "Mostly Likely Options : {'task-invariant'} \n",
      "\n",
      "\n",
      "sharable -> mostly liked answer : shareable \n",
      "Mostly Likely Options : {'shareable'} \n",
      "\n",
      "\n",
      "task-dependent -> mostly liked answer : task-dependent \n",
      "Mostly Likely Options : {'task-dependent'} \n",
      "\n",
      "\n",
      "luong -> mostly liked answer : long \n",
      "Mostly Likely Options : {'long', 'loong', 'lung', 'tuong', 'liong'} \n",
      "\n",
      "\n",
      "shared-private -> mostly liked answer : shared-private \n",
      "Mostly Likely Options : {'shared-private'} \n",
      "\n",
      "\n",
      "�infantile� -> mostly liked answer : infantile \n",
      "Mostly Likely Options : {'infantile'} \n",
      "\n",
      "\n",
      "multi-task -> mostly liked answer : multi-track \n",
      "Mostly Likely Options : {'multi-track', 'multi-stack'} \n",
      "\n",
      "\n",
      "neural-based -> mostly liked answer : neural-based \n",
      "Mostly Likely Options : {'neural-based'} \n",
      "\n",
      "\n",
      "herently -> mostly liked answer : recently \n",
      "Mostly Likely Options : {'recently', 'fervently', 'coherently', 'inherently', 'decently', 'serenely'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCorrection(researchTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context sensitive word correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "word_length = 2\n",
    "prefix_length = 7\n",
    "sym_spell = SymSpell(word_length, prefix_length)\n",
    "print(\"Corpus file not found\") if not sym_spell.create_dictionary(\"words.txt\") else print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = 10\n",
    "def correct_tocknized_text(words):\n",
    "    corr_count = 0\n",
    "    corrected_words = []\n",
    "    for i, word in enumerate(words[:-word_length+1]):\n",
    "        word_set = [words[i+j] for j in range(word_length)]\n",
    "        _input = ' '.join(word_set)\n",
    "        result = sym_spell.word_segmentation(_input)\n",
    "        correction = result.corrected_string\n",
    "        if correction.lower() != _input.lower() and preview < corr_count:\n",
    "            corr_count += 1\n",
    "            print('\"{}\" is corrected as \"{}\"'.format(_input, correction))\n",
    "        corrected_words.append(correction.split(' ')[0])\n",
    "    corrected_words.append(correction.split(' ')[1])\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Frauds', 'Monsey', 'avoiding', 'deportation', 'from', 'Canada', 'a', 'cdn', 'lpc', 'cp', 'a', 'a', 'a', 'ttp', 'immigration', 'integration', 'canada', 'ttp', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'Uk', 'economy', 'a', 'Same', 'as', 'Australia', 'a', 'Canada', 'a', 'ttp', 'Is', 'the', 'new', 'Manitoba', 'immigration', 'fee', 'a', 'head', 'tax', 'a', 'ttp', 'Canada', 'immigration', 'profit', 'influence', 'modernistic', 'delhi', 'yet', 'abhinaya', 'a', 'Azofy', 'ttp', 'Canada', 'Immigration', 'Minister', 'to', 'a', 'a', 'a', 'Substantially', 'Increase', 'Immigration', 'Numbers', 'hot', 'hot', 'M', 'a', 'a', 'me', 'les', 'dusa', 'up', 'ays', 'dim', 'par', 'excellence', 'Contr', 'a', 'a', 'Le', 'Rigour', 'immigration', 'et', 'acc', 'a', 'a', 's', 'a', 'a', 'la', 'greenyard', 'a', 'a', 'a', 'a', 'ttp', 'pshaw', 'what', 'changes', 'should', 'be', 'made', 'to', 'Canada', 'immigration', 'laws', 'due', 'to', 'the', 'influx', 'of', 'immigration', 'and', 'violence', 'a', 'L', 'a', 'a', 'immigration', 'irra', 'a', 'a', 'gulix', 'a', 'a', 're', 'au', 'Canada', 'd', 'a', 'a', 'corti', 'a', 'a', 'e', 'en', '5', 'questions', 'ttp', 'Immigration', 'irra', 'a', 'a', 'gulix', 'a', 'a', 're', 'au', 'Canada', 'd', 'a', 'a', 'corti', 'a', 'a', 'e', 'en', '5', 'questions', 'ttp', 'ttp', 'ttp', 'ttp', 'Will', 'Media', 'ask', 'the', 'Liberals', 'if', 'they', 'actually', 'have', 'a', 'solid', 'plan', 'for', 'Canada', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'From', 'my', 'view', 'a', 'immigration', 'immigration', 'out', 'of', 'C', 'a', 'a', 'a', 'ttp', 'Dan', 'Murray', 'of', 'a', 'a', 'Immigration', 'Watch', 'Canada', 'is', 'xenophobic', 'racist', 'fear', 'liar', 'racism', 'canada', 'cdn', 'hater', 'a', 'a', 'a', 'ttp', 'Le', 'Canada', 'lance', 'une', 'caste', 'campagne', 'dim', 'pour', 'faire', 'face', 'a', 'a', 'son', 'besoin', 'de', 'main', \"d'\", \"'d\", 'a', 'a', 'uvre', 'hot', 'L', 'a', 'a', 'immigration', 'irra', 'a', 'a', 'gulix', 'a', 'a', 're', 'au', 'canada', 'd', 'a', 'a', 'corti', 'a', 'a', 'e', 'en', '5', 'a', 'a', 'questions', 'ttp', 'candidly', \"I've\", 'read', 'the', 'Immigration', 'laws', 'of', 'Canada', 'much', 'stricter', 'than', 'the', 'Us', 'Canada', 'Immigration', 'Web', 'Traffic', 'Surges', 'And', 'Crashes', 'In', 'Wake', 'Of', 'Trump', 'fast', 'a', 'site', 'a', 'web', 'a', 'traffic', 'ttp', 'Mr', 'Know', 'of', 'Canada', 'Immigration', 'ttp', 'Move', 'to', 'Canada', 'lady', 'Oh', 'a', 'immigration', 'rules', 'a', 'you', \"can't\", '...', 'ttp', 'orthis', 'Annette', 'Toft', 'becomes', 'Canada', '2', 'millionth', 'immigrant', 'since', '14', 'a', 'Do', 'you', 'know', 'your', \"family's\", 'immigration', 'st', 'a', 'a', 'a', 'ttp', 'a', 'the', 'profiles', 'Canada', 'open', 'immigration', 'policies', 'a', 'how', 'they', 'contribute', 'to', 'our', 'economic', 'success', 'a', 'a', 'a', 'a', 'ttp', 'Hundreds', 'may', 'lose', 'Canadian', 'citizenship', 'a', 'resident', 'status', 'because', 'of', 'one', 'corrupt', 'immigration', 'consultant', 'ttp', 'Immigration', 'for', 'canada', 'without', 'india', 'a', 'an', 'compassionate', 'handle', 'a', 'deify', 'a', 'jamaican', 'immigrants', 'canada', 'ttp', 'statistics', 'immigration', 'a', 'Mexican', 'visa', 'lift', 'expected', 'to', 'cost', 'Canada', 'a', '22', 'over', 'a', 'decade', 'ttp', 'Are', 'people', 'still', 'moving', 'to', 'canada', 'a', 'a', 'a', 'Oh', \"that's\", 'right', 'a', 'they', 'have', 'real', 'immigration', 'laws', 'and', \"it's\", 'a', 'a', 'a', 'ttp', 'Here', 'are', 'more', 'details', 'on', 'the', 'Richmond', 'a', 'B', 'a', 'C', 'a', 'Immigration', 'Consultant', 'Sunny', 'Wang', 'who', 'was', 'sentenced', 'to', '7', 'years', 'in', '...', 'ttp', 'I', 'added', 'a', 'video', 'to', 'a', 'you', 'playlist', 'ttp', 'Funny', 'Talking', 'of', 'Haryana', 'Jat', 'with', 'Canada', 'Immigration', 'Girl', 'Agent', 'Mexicans', 'Can', 'Now', 'Travel', 'Visa', 'To', 'Canada', 'ttp', 'ttp', 'L', 'a', 'a', 'immigration', 'irra', 'a', 'a', 'gulix', 'a', 'a', 're', 'au', 'Canada', 'd', 'a', 'a', 'corti', 'a', 'a', 'e', 'en', '5', 'a', 'a', 'questions', 'ttp', 'sweetness', 'Hes', 'the', 'Pos', 'that', 'ramped', 'up', 'immigration', 'for', 'Canada', 'a', 'among', 'other', 'globalist', 'policies', 'a', 'Canada', 'lifted', 'visa', 'requirements', 'to', 'Mexico', 'as', 'of', 'Dec', '1', 'a', '20', 'a', 'Thoughts', 'a', 'visa', 'immigration', 'chuffing', 'people', 'Keep', 'praising', 'Canada', 'and', 'Canada', 'has', 'way', 'stricter', 'immigration', 'laws', 'then', 'us', 'they', 'will', 'boot', 'your', 'liberal', 'American', 'ass']\n"
     ]
    }
   ],
   "source": [
    "print(correct_tocknized_text(twitterTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feedback data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', 'a', 'Lectures', 'are', 'understandable', 'a', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'self', 'also', 'a', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', 'a', 're', 'Good', 'a', 'a', 'a', 'br', 'a', 'a', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', 'a', 'a', '9', 'a', 's', 'better', 'for', 'us', 'a', 'a', 'br', 'a', 'a', 'sometimes', 'teaching', 'speed', 'is', 'very', 'high', 'a', 'a', 'br', 'a', 'a', 'a', 'br', 'a', 'a', 'Thanks', 'a', 'a', 'a', 'a', 'br', 'a', 'a', 're', 'The', 'lectures', 'are', 'good', 'a', 'bit', 'speed', 'in', 'class', 'working', 'activity', 'is', 'a', 'must', 'oneco', 'please', 'take', 'another', 'hour', 'in', 'thursdays', 'madame', \"o'\", 'a', 'br', 'a', 'a', 'We', 'can', 'hear', 'your', 'voice', 'clearly', 'and', 'can', 'understand', 'the', 'things', 'you', 'teach', 'a', 'Presentation', 'slides', 'also', 'good', 'source', 'to', 'refer', 'a', 'lf', 'you', 'can', 'do', 'more', 'example', 'questions', 'within', 'the', 'classroom', 'and', 'it', 'will', 'help', 'us', 'to', 'understand', 'the', 'principles', 'well', 'a', 'a', 'br', 'a', 'a', \"o'\", 'Lectures', 'was', 'well', 'structured', 'and', 'well', 'organized', 'a', 'It', 'was', 'easy', 'to', 'understand', 'a', 'Lecture', 'slides', 'and', 'labs', 'were', 'also', 'well', 'organized', 'a', 'Lectures', 'were', 'good', 'a', 'understandable', 'a', 'The', 'lecture', 'slides', 'were', 'well', 'organized', 'and', 'the', 'examples', 'done', 'in', 'the', 'class', 'helped', 'a', 'lot', 'to', 'learn', 'this', 'new', 'language', 'and', 'also', 'the', 'principles', 'of', 'Oop', 'a', 'Motivated', 'to', 'well', 'a', 'Would', 'have', 'been', 'better', 'if', 'we', 'discussed', 'more', 'about', 'the', 'solutions', 'of', 'coding', 'exercisers', 'a', 'I', 'think', 'i', 'learned', 'a', 'lot', 'from', 'the', 'codes', 'you', 'write', 'in', 'the', 'board', 'a', 'When', 'i', 'compare', 'my', 'codes', 'with', 'yours', 'i', 'can', 'learn', 'about', 'my', 'mistakes', 'and', 'good', 'coding', 'practices', 'that', 'i', 'should', 'follow', 'a', 'There', 'fore', 'i', 'think', 'it', 'would', 'be', 'great', 'if', 'we', 'can', 'discuss', 'more', 'examples', 'in', 'the', 'class', 'a', 'madam', 'explained', 'the', 'oop', 'concepts', 'clearly', 'with', 'examples', 'were', 'interesting', 'want', 'more', 'scenario', 'examples', 'and', 'answers', 'with', 'explanations', 'in', 'future', 'a', 'I', 'satisfy', 'about', 'first', '7', 'lectures', 'a', 'That', 'way', 'of', 'teaching', 'is', 'really', 'good', 'for', 'coming', 'lectures', 'too', 'a', 'lecturers', 'are', 'very', 'good', 'a', 'take', 'good', 'effort', 'to', 'make', 'understand', 'every', 'student', 'in', 'the', 'room', 'a', 'very', 'helpfully', 'a', 'I', 'was', 'able', 'to', 'obtain', 'a', 'clear', 'picture', 'about', 'Oop', 'and', 'its', 'concepts', 'a', 're', 'lecture', 'slides', 'a', 'explanations', 'were', 'very', 'clear', 'a', 'a', 'br', 'a', 'a', 'it', 'a', 'a', '9', 'a', 's', 'very', 'good', 'to', 'letting', 'ask', 'questions', 'and', 'explain', 'again', 'with', 'suitable', 'examples', 'a', 'a', 'br', 'a', 'a', 'sometimes', 'a', 'some', 'codes', 'on', 'white', 'board', 'were', 'unclear', 'at', 'the', 'back', 'a', 'a', 'br', 'a', 'a', 'overall', 'very', 'good', 'a', 'a', 'a', 'a', 'br', 'a', 'a', \"o'\", 'The', 'lectures', 'were', 'good', 'and', 'clear', 'a', 'And', 'they', 'weren', 'a', 'a', '9', 'a', 't', 'too', 'fast', 'a', 'Writing', 'code', 'was', 'somewhat', 'confusing', 'because', 'I', 'didn', 'a', 'a', '9', 'a', 't', 'know', 'java', 'before', 'a', 'Actually', 'teaching', 'is', 'very', 'good', 'and', 'can', 'understand', 'easily', 'the', 'concepts', 'by', 'examples', 'which', 'are', 'given', 'in', 'the', 'classic', 'will', 'be', 'more', 'helpful', 'if', 'provide', 'solved', 'questions', 'as', 'well', 'a', 'a', 'thankyou']\n"
     ]
    }
   ],
   "source": [
    "print(correct_tocknized_text(feedbackTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#research paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multitask', 'learning', 'a', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task', 'features', 'a', 'However', 'a', 'in', 'most', 'existing', 'approaches', 'a', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'tasks', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', 'a', 'In', 'this', 'paper', 'a', 'we', 'propose', 'an', 'adversarial', 'multitask', 'learning', 'framework', 'a', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', 'a', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', 'a', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', 'a', 'Besides', 'a', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', 'a', 'Multitask', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', 'a', 'Recently', 'a', 'neural', 'models', 'for', 'multitask', 'learning', 'have', 'become', 'very', 'popular', 'a', 'ranging', 'from', 'computer', 'vision', 'a', 'Misura', 'et', 'all', 'a', '20', 'a', 'Zhang', 'et', 'all', 'a', '20', 'a', 'to', 'natural', 'language', 'processing', 'a', 'Collibert', 'and', 'a', '1080', 'a', 'Long', 'et', 'all', 'a', '25', 'a', 'a', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', 'a', 'However', 'a', 'most', 'existing', 'work', 'on', 'multitask', 'learning', 'a', 'Liu', 'et', 'all', 'a', '20', 'a', 'b', 'a', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', 'a', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', 'a', 'As', 'shown', 'in', 'Figure', '1', 'a', 'a', 'a', 'a', 'the', 'general', 'shared', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', 'a', 'one', 'is', 'used', 'to', 'store', 'task', 'features', 'a', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', 'a', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'tasks', 'features', 'a', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', 'a', 'suffering', 'from', 'feature', 'redundancy', 'a', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', 'a', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', 'a', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', 'a', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', 'a', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', 'a', 'The', 'word', 'infantile', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', 'a', 'However', 'a', 'the', 'general', 'shared', 'model', 'could', 'place', 'the', 'tasks', 'word', 'infantile', 'in', 'a', 'shared', 'space', 'a', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', 'a', 'Additionally', 'a', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', 'a', 'To', 'address', 'this', 'problem', 'a', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multitask', 'framework', 'a', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'inherently', 'recently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints', 'a', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(correct_tocknized_text(researchTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rasika_105127\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Reminds : Reminds\n",
      "Lemma for me : me\n",
      "Lemma for of : of\n",
      "Lemma for Liberal : Liberal\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Fraudster : Fraudster\n",
      "Lemma for Monsef : Monsef\n",
      "Lemma for avoiding : avoiding\n",
      "Lemma for deportation : deportation\n",
      "Lemma for from : from\n",
      "Lemma for Canada : Canada\n",
      "Lemma for . : .\n",
      "Lemma for # : #\n",
      "Lemma for cdnpoli : cdnpoli\n",
      "Lemma for # : #\n",
      "Lemma for LPC : LPC\n",
      "Lemma for # : #\n",
      "Lemma for CPCLDR��_ : CPCLDR��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/ZOZOSe1CqQ : //t.co/ZOZOSe1CqQ\n",
      "Lemma for # : #\n",
      "Lemma for immigration : immigration\n",
      "Lemma for # : #\n",
      "Lemma for integration : integration\n",
      "Lemma for # : #\n",
      "Lemma for canada : canada\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/M5cKGyvV8F : //t.co/M5cKGyvV8F\n",
      "Lemma for We : We\n",
      "Lemma for want : want\n",
      "Lemma for controlled : controlled\n",
      "Lemma for immigration : immigration\n",
      "Lemma for that : that\n",
      "Lemma for contributes : contributes\n",
      "Lemma for positively : positively\n",
      "Lemma for to : to\n",
      "Lemma for the : the\n",
      "Lemma for UK : UK\n",
      "Lemma for economy : economy\n",
      "Lemma for . : .\n",
      "Lemma for Same : Same\n",
      "Lemma for as : a\n",
      "Lemma for Australia : Australia\n",
      "Lemma for & : &\n",
      "Lemma for amp : amp\n",
      "Lemma for ; : ;\n",
      "Lemma for Canada : Canada\n",
      "Lemma for . : .\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/99mYliuOes : //t.co/99mYliuOes\n",
      "Lemma for Is : Is\n",
      "Lemma for the : the\n",
      "Lemma for new : new\n",
      "Lemma for Manitoba : Manitoba\n",
      "Lemma for immigration : immigration\n",
      "Lemma for fee : fee\n",
      "Lemma for a : a\n",
      "Lemma for head : head\n",
      "Lemma for tax : tax\n",
      "Lemma for ? : ?\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/LsG7C3vLe9 : //t.co/LsG7C3vLe9\n",
      "Lemma for Canada : Canada\n",
      "Lemma for immigration : immigration\n",
      "Lemma for profit : profit\n",
      "Lemma for influence : influence\n",
      "Lemma for modernistic : modernistic\n",
      "Lemma for delhi : delhi\n",
      "Lemma for yet : yet\n",
      "Lemma for abhinav : abhinav\n",
      "Lemma for : : :\n",
      "Lemma for XKofy : XKofy\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/becgusY2i6 : //t.co/becgusY2i6\n",
      "Lemma for Canada : Canada\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Minister : Minister\n",
      "Lemma for to : to\n",
      "Lemma for ���Substantially : ���Substantially\n",
      "Lemma for Increase : Increase\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Numbers : Numbers\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/nEFw30MRaa : //t.co/nEFw30MRaa\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/cyI867PZRV : //t.co/cyI867PZRV\n",
      "Lemma for M��me : M��me\n",
      "Lemma for les : le\n",
      "Lemma for # : #\n",
      "Lemma for USA=pays : USA=pays\n",
      "Lemma for d'immigration : d'immigration\n",
      "Lemma for par : par\n",
      "Lemma for excellence : excellence\n",
      "Lemma for CONTR��LE : CONTR��LE\n",
      "Lemma for RIGOUREUSEMENT : RIGOUREUSEMENT\n",
      "Lemma for l'immigration : l'immigration\n",
      "Lemma for et : et\n",
      "Lemma for acc��s : acc��s\n",
      "Lemma for �� : ��\n",
      "Lemma for la : la\n",
      "Lemma for # : #\n",
      "Lemma for GreenCARD : GreenCARD\n",
      "Lemma for ! : !\n",
      "Lemma for ��_ : ��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/IHpVhW2BaG : //t.co/IHpVhW2BaG\n",
      "Lemma for @ : @\n",
      "Lemma for Shawhelp : Shawhelp\n",
      "Lemma for what : what\n",
      "Lemma for changes : change\n",
      "Lemma for should : should\n",
      "Lemma for be : be\n",
      "Lemma for made : made\n",
      "Lemma for to : to\n",
      "Lemma for Canada : Canada\n",
      "Lemma for 's : 's\n",
      "Lemma for immigration : immigration\n",
      "Lemma for laws : law\n",
      "Lemma for due : due\n",
      "Lemma for to : to\n",
      "Lemma for the : the\n",
      "Lemma for influx : influx\n",
      "Lemma for of : of\n",
      "Lemma for immigration : immigration\n",
      "Lemma for and : and\n",
      "Lemma for violence : violence\n",
      "Lemma for ? : ?\n",
      "Lemma for L��immigration : L��immigration\n",
      "Lemma for irr��guli��re : irr��guli��re\n",
      "Lemma for au : au\n",
      "Lemma for Canada : Canada\n",
      "Lemma for d��cortiqu��e : d��cortiqu��e\n",
      "Lemma for en : en\n",
      "Lemma for 5 : 5\n",
      "Lemma for questions : question\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/f4utO5A7ZF : //t.co/f4utO5A7ZF\n",
      "Lemma for L'immigration : L'immigration\n",
      "Lemma for irr��guli��re : irr��guli��re\n",
      "Lemma for au : au\n",
      "Lemma for Canada : Canada\n",
      "Lemma for d��cortiqu��e : d��cortiqu��e\n",
      "Lemma for en : en\n",
      "Lemma for 5 : 5\n",
      "Lemma for questions : question\n",
      "Lemma for - : -\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/UiBsEZOqas : //t.co/UiBsEZOqas\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/j77dEvjoiX : //t.co/j77dEvjoiX\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/XXDeIG7Dbu : //t.co/XXDeIG7Dbu\n",
      "Lemma for Will : Will\n",
      "Lemma for Media : Media\n",
      "Lemma for ask : ask\n",
      "Lemma for the : the\n",
      "Lemma for Liberals : Liberals\n",
      "Lemma for if : if\n",
      "Lemma for they : they\n",
      "Lemma for actually : actually\n",
      "Lemma for have : have\n",
      "Lemma for a : a\n",
      "Lemma for solid : solid\n",
      "Lemma for plan : plan\n",
      "Lemma for for : for\n",
      "Lemma for Canada : Canada\n",
      "Lemma for _��_�_ : _��_�_\n",
      "Lemma for ? : ?\n",
      "Lemma for ? : ?\n",
      "Lemma for From : From\n",
      "Lemma for my : my\n",
      "Lemma for view : view\n",
      "Lemma for -- : --\n",
      "Lemma for immigration : immigration\n",
      "Lemma for out : out\n",
      "Lemma for of : of\n",
      "Lemma for C��_ : C��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/YAgwmZ8ECp : //t.co/YAgwmZ8ECp\n",
      "Lemma for Dan : Dan\n",
      "Lemma for Murray : Murray\n",
      "Lemma for of��Immigration : of��Immigration\n",
      "Lemma for Watch : Watch\n",
      "Lemma for Canada : Canada\n",
      "Lemma for is : is\n",
      "Lemma for xenophobic : xenophobic\n",
      "Lemma for racist : racist\n",
      "Lemma for fear-mongering : fear-mongering\n",
      "Lemma for liar : liar\n",
      "Lemma for # : #\n",
      "Lemma for racism : racism\n",
      "Lemma for # : #\n",
      "Lemma for canada : canada\n",
      "Lemma for # : #\n",
      "Lemma for cdnpoli : cdnpoli\n",
      "Lemma for # : #\n",
      "Lemma for hatecrime��_ : hatecrime��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/kwZ3csvYxM : //t.co/kwZ3csvYxM\n",
      "Lemma for Le : Le\n",
      "Lemma for Canada : Canada\n",
      "Lemma for lance : lance\n",
      "Lemma for une : une\n",
      "Lemma for vaste : vaste\n",
      "Lemma for campagne : campagne\n",
      "Lemma for d'immigration : d'immigration\n",
      "Lemma for pour : pour\n",
      "Lemma for faire : faire\n",
      "Lemma for face : face\n",
      "Lemma for �� : ��\n",
      "Lemma for son : son\n",
      "Lemma for besoin : besoin\n",
      "Lemma for de : de\n",
      "Lemma for main : main\n",
      "Lemma for d'��uvre : d'��uvre\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/kXdfMGTZzN : //t.co/kXdfMGTZzN\n",
      "Lemma for L�� : L��\n",
      "Lemma for # : #\n",
      "Lemma for immigration : immigration\n",
      "Lemma for irr��guli��re : irr��guli��re\n",
      "Lemma for au : au\n",
      "Lemma for # : #\n",
      "Lemma for Canada : Canada\n",
      "Lemma for d��cortiqu��e : d��cortiqu��e\n",
      "Lemma for en : en\n",
      "Lemma for 5��questions : 5��questions\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/s3hu1OKKIG : //t.co/s3hu1OKKIG\n",
      "Lemma for @ : @\n",
      "Lemma for Canadidly : Canadidly\n",
      "Lemma for I : I\n",
      "Lemma for 've : 've\n",
      "Lemma for read : read\n",
      "Lemma for the : the\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for laws : law\n",
      "Lemma for of : of\n",
      "Lemma for Canada : Canada\n",
      "Lemma for much : much\n",
      "Lemma for stricter : stricter\n",
      "Lemma for than : than\n",
      "Lemma for the : the\n",
      "Lemma for US : US\n",
      "Lemma for Canada : Canada\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Website : Website\n",
      "Lemma for Traffic : Traffic\n",
      "Lemma for Surges : Surges\n",
      "Lemma for And : And\n",
      "Lemma for Crashes : Crashes\n",
      "Lemma for In : In\n",
      "Lemma for Wake : Wake\n",
      "Lemma for Of : Of\n",
      "Lemma for Trump : Trump\n",
      "Lemma for # : #\n",
      "Lemma for fasttraffic : fasttraffic\n",
      "Lemma for , : ,\n",
      "Lemma for # : #\n",
      "Lemma for sitetraffic : sitetraffic\n",
      "Lemma for , : ,\n",
      "Lemma for # : #\n",
      "Lemma for website : website\n",
      "Lemma for , : ,\n",
      "Lemma for # : #\n",
      "Lemma for traffic : traffic\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/zRlJ26jnkC : //t.co/zRlJ26jnkC\n",
      "Lemma for Mr : Mr\n",
      "Lemma for Know-all : Know-all\n",
      "Lemma for of : of\n",
      "Lemma for Canada : Canada\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/wTQK4QDiKI : //t.co/wTQK4QDiKI\n",
      "Lemma for Move : Move\n",
      "Lemma for to : to\n",
      "Lemma for Canada : Canada\n",
      "Lemma for @ : @\n",
      "Lemma for LadyMadonna___ : LadyMadonna___\n",
      "Lemma for Oh : Oh\n",
      "Lemma for , : ,\n",
      "Lemma for immigration : immigration\n",
      "Lemma for rules : rule\n",
      "Lemma for , : ,\n",
      "Lemma for you : you\n",
      "Lemma for ca : ca\n",
      "Lemma for n't : n't\n",
      "Lemma for ... : ...\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/5LIEVHO7A4 : //t.co/5LIEVHO7A4\n",
      "Lemma for # : #\n",
      "Lemma for OnThisDay : OnThisDay\n",
      "Lemma for Annette : Annette\n",
      "Lemma for Toft : Toft\n",
      "Lemma for becomes : becomes\n",
      "Lemma for Canada : Canada\n",
      "Lemma for 's : 's\n",
      "Lemma for 2 : 2\n",
      "Lemma for millionth : millionth\n",
      "Lemma for immigrant : immigrant\n",
      "Lemma for since : since\n",
      "Lemma for 1945 : 1945\n",
      "Lemma for . : .\n",
      "Lemma for Do : Do\n",
      "Lemma for you : you\n",
      "Lemma for know : know\n",
      "Lemma for your : your\n",
      "Lemma for family : family\n",
      "Lemma for 's : 's\n",
      "Lemma for immigration : immigration\n",
      "Lemma for st��_ : st��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/UvRuw8eR1b : //t.co/UvRuw8eR1b\n",
      "Lemma for . : .\n",
      "Lemma for @ : @\n",
      "Lemma for TheEconomist : TheEconomist\n",
      "Lemma for profiles : profile\n",
      "Lemma for Canada : Canada\n",
      "Lemma for 's : 's\n",
      "Lemma for open : open\n",
      "Lemma for immigration : immigration\n",
      "Lemma for policies : policy\n",
      "Lemma for & : &\n",
      "Lemma for amp : amp\n",
      "Lemma for ; : ;\n",
      "Lemma for how : how\n",
      "Lemma for they : they\n",
      "Lemma for contribute : contribute\n",
      "Lemma for to : to\n",
      "Lemma for our : our\n",
      "Lemma for economic : economic\n",
      "Lemma for success : success\n",
      "Lemma for : : :\n",
      "Lemma for ��_ : ��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/4K84EE8Y63 : //t.co/4K84EE8Y63\n",
      "Lemma for Hundreds : Hundreds\n",
      "Lemma for may : may\n",
      "Lemma for lose : lose\n",
      "Lemma for Canadian : Canadian\n",
      "Lemma for citizenship : citizenship\n",
      "Lemma for , : ,\n",
      "Lemma for resident : resident\n",
      "Lemma for status : status\n",
      "Lemma for because : because\n",
      "Lemma for of : of\n",
      "Lemma for one : one\n",
      "Lemma for corrupt : corrupt\n",
      "Lemma for immigration : immigration\n",
      "Lemma for consultant : consultant\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/x2IfO0EXI2 : //t.co/x2IfO0EXI2\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for for : for\n",
      "Lemma for canada : canada\n",
      "Lemma for without : without\n",
      "Lemma for india : india\n",
      "Lemma for : : :\n",
      "Lemma for an : an\n",
      "Lemma for compassionate : compassionate\n",
      "Lemma for handle : handle\n",
      "Lemma for : : :\n",
      "Lemma for deyFy : deyFy\n",
      "Lemma for '' : ''\n",
      "Lemma for # : #\n",
      "Lemma for Jamaican : Jamaican\n",
      "Lemma for # : #\n",
      "Lemma for immigrants : immigrant\n",
      "Lemma for # : #\n",
      "Lemma for Canada : Canada\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/vcmfYGadR5 : //t.co/vcmfYGadR5\n",
      "Lemma for # : #\n",
      "Lemma for statistics : statistic\n",
      "Lemma for # : #\n",
      "Lemma for immigration : immigration\n",
      "Lemma for '' : ''\n",
      "Lemma for Mexican : Mexican\n",
      "Lemma for visa : visa\n",
      "Lemma for lift : lift\n",
      "Lemma for expected : expected\n",
      "Lemma for to : to\n",
      "Lemma for cost : cost\n",
      "Lemma for Canada : Canada\n",
      "Lemma for $ : $\n",
      "Lemma for 262M : 262M\n",
      "Lemma for over : over\n",
      "Lemma for a : a\n",
      "Lemma for decade : decade\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/9i72fRhtij : //t.co/9i72fRhtij\n",
      "Lemma for Are : Are\n",
      "Lemma for people : people\n",
      "Lemma for still : still\n",
      "Lemma for moving : moving\n",
      "Lemma for to : to\n",
      "Lemma for # : #\n",
      "Lemma for Canada : Canada\n",
      "Lemma for ? : ?\n",
      "Lemma for ? : ?\n",
      "Lemma for ? : ?\n",
      "Lemma for Oh : Oh\n",
      "Lemma for that : that\n",
      "Lemma for 's : 's\n",
      "Lemma for right : right\n",
      "Lemma for , : ,\n",
      "Lemma for they : they\n",
      "Lemma for have : have\n",
      "Lemma for real : real\n",
      "Lemma for immigration : immigration\n",
      "Lemma for laws : law\n",
      "Lemma for and : and\n",
      "Lemma for it's��_ : it's��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/0C5OBfmxLG : //t.co/0C5OBfmxLG\n",
      "Lemma for Here : Here\n",
      "Lemma for are : are\n",
      "Lemma for more : more\n",
      "Lemma for details : detail\n",
      "Lemma for on : on\n",
      "Lemma for the : the\n",
      "Lemma for Richmond : Richmond\n",
      "Lemma for , : ,\n",
      "Lemma for B.C : B.C\n",
      "Lemma for . : .\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Consultant : Consultant\n",
      "Lemma for Sunny : Sunny\n",
      "Lemma for Wang : Wang\n",
      "Lemma for who : who\n",
      "Lemma for was : wa\n",
      "Lemma for sentenced : sentenced\n",
      "Lemma for to : to\n",
      "Lemma for 7 : 7\n",
      "Lemma for years : year\n",
      "Lemma for in : in\n",
      "Lemma for ... : ...\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/YXH5W53srO : //t.co/YXH5W53srO\n",
      "Lemma for I : I\n",
      "Lemma for added : added\n",
      "Lemma for a : a\n",
      "Lemma for video : video\n",
      "Lemma for to : to\n",
      "Lemma for a : a\n",
      "Lemma for @ : @\n",
      "Lemma for YouTube : YouTube\n",
      "Lemma for playlist : playlist\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/CnEyWN40x3 : //t.co/CnEyWN40x3\n",
      "Lemma for Funny : Funny\n",
      "Lemma for Talking : Talking\n",
      "Lemma for of : of\n",
      "Lemma for Haryanavi : Haryanavi\n",
      "Lemma for Jat : Jat\n",
      "Lemma for with : with\n",
      "Lemma for Canada : Canada\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Girl : Girl\n",
      "Lemma for Agent : Agent\n",
      "Lemma for Mexicans : Mexicans\n",
      "Lemma for Can : Can\n",
      "Lemma for Now : Now\n",
      "Lemma for Travel : Travel\n",
      "Lemma for Visa-Free : Visa-Free\n",
      "Lemma for To : To\n",
      "Lemma for Canada : Canada\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/Ec3XHORO2s : //t.co/Ec3XHORO2s\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/RQRr5nebcG : //t.co/RQRr5nebcG\n",
      "Lemma for L��immigration : L��immigration\n",
      "Lemma for irr��guli��re : irr��guli��re\n",
      "Lemma for au : au\n",
      "Lemma for Canada : Canada\n",
      "Lemma for d��cortiqu��e : d��cortiqu��e\n",
      "Lemma for en : en\n",
      "Lemma for 5��questions : 5��questions\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/DkpuKyWmaK : //t.co/DkpuKyWmaK\n",
      "Lemma for @ : @\n",
      "Lemma for SweetnessShawnB : SweetnessShawnB\n",
      "Lemma for Hes : Hes\n",
      "Lemma for the : the\n",
      "Lemma for POS : POS\n",
      "Lemma for that : that\n",
      "Lemma for ramped : ramped\n",
      "Lemma for up : up\n",
      "Lemma for immigration : immigration\n",
      "Lemma for for : for\n",
      "Lemma for Canada : Canada\n",
      "Lemma for , : ,\n",
      "Lemma for among : among\n",
      "Lemma for other : other\n",
      "Lemma for globalist : globalist\n",
      "Lemma for policies : policy\n",
      "Lemma for . : .\n",
      "Lemma for Canada : Canada\n",
      "Lemma for lifted : lifted\n",
      "Lemma for visa : visa\n",
      "Lemma for requirements : requirement\n",
      "Lemma for to : to\n",
      "Lemma for Mexico : Mexico\n",
      "Lemma for as : a\n",
      "Lemma for of : of\n",
      "Lemma for Dec : Dec\n",
      "Lemma for 1 : 1\n",
      "Lemma for , : ,\n",
      "Lemma for 2016 : 2016\n",
      "Lemma for . : .\n",
      "Lemma for Thoughts : Thoughts\n",
      "Lemma for ? : ?\n",
      "Lemma for # : #\n",
      "Lemma for visa : visa\n",
      "Lemma for # : #\n",
      "Lemma for immigration : immigration\n",
      "Lemma for @ : @\n",
      "Lemma for HuffingtonPost : HuffingtonPost\n",
      "Lemma for people : people\n",
      "Lemma for Keep : Keep\n",
      "Lemma for praising : praising\n",
      "Lemma for Canada : Canada\n",
      "Lemma for and : and\n",
      "Lemma for Canada : Canada\n",
      "Lemma for has : ha\n",
      "Lemma for way : way\n",
      "Lemma for stricter : stricter\n",
      "Lemma for immigration : immigration\n",
      "Lemma for laws : law\n",
      "Lemma for then : then\n",
      "Lemma for us : u\n",
      "Lemma for they : they\n",
      "Lemma for willl : willl\n",
      "Lemma for boot : boot\n",
      "Lemma for your : your\n",
      "Lemma for liberal : liberal\n",
      "Lemma for American : American\n",
      "Lemma for ass : as\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenization = nltk.word_tokenize(twitterdata)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} : {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Honestly : Honestly\n",
      "Lemma for last : last\n",
      "Lemma for seven : seven\n",
      "Lemma for lectures : lecture\n",
      "Lemma for are : are\n",
      "Lemma for good : good\n",
      "Lemma for . : .\n",
      "Lemma for Lectures : Lectures\n",
      "Lemma for are : are\n",
      "Lemma for understandable : understandable\n",
      "Lemma for . : .\n",
      "Lemma for Lecture : Lecture\n",
      "Lemma for slides : slide\n",
      "Lemma for are : are\n",
      "Lemma for very : very\n",
      "Lemma for useful : useful\n",
      "Lemma for to : to\n",
      "Lemma for self-study : self-study\n",
      "Lemma for also : also\n",
      "Lemma for . : .\n",
      "Lemma for The : The\n",
      "Lemma for given : given\n",
      "Lemma for opportunity : opportunity\n",
      "Lemma for to : to\n",
      "Lemma for ask : ask\n",
      "Lemma for questions : question\n",
      "Lemma for from : from\n",
      "Lemma for the : the\n",
      "Lemma for lecturer : lecturer\n",
      "Lemma for is : is\n",
      "Lemma for appreciative : appreciative\n",
      "Lemma for . : .\n",
      "Lemma for `` : ``\n",
      "Lemma for Good : Good\n",
      "Lemma for : : :\n",
      "Lemma for ) : )\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for please : please\n",
      "Lemma for do : do\n",
      "Lemma for recap : recap\n",
      "Lemma for at : at\n",
      "Lemma for class : class\n",
      "Lemma for starting : starting\n",
      "Lemma for it : it\n",
      "Lemma for & : &\n",
      "Lemma for # : #\n",
      "Lemma for 039 : 039\n",
      "Lemma for ; : ;\n",
      "Lemma for s : s\n",
      "Lemma for better : better\n",
      "Lemma for for : for\n",
      "Lemma for us : u\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for sometimes : sometimes\n",
      "Lemma for teaching : teaching\n",
      "Lemma for speed : speed\n",
      "Lemma for is : is\n",
      "Lemma for very : very\n",
      "Lemma for high : high\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for Thanks : Thanks\n",
      "Lemma for ! : !\n",
      "Lemma for : : :\n",
      "Lemma for ) : )\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for `` : ``\n",
      "Lemma for The : The\n",
      "Lemma for lectures : lecture\n",
      "Lemma for are : are\n",
      "Lemma for good..but : good..but\n",
      "Lemma for a : a\n",
      "Lemma for bit : bit\n",
      "Lemma for speed.A : speed.A\n",
      "Lemma for in : in\n",
      "Lemma for class : class\n",
      "Lemma for working : working\n",
      "Lemma for activity : activity\n",
      "Lemma for is : is\n",
      "Lemma for a : a\n",
      "Lemma for must : must\n",
      "Lemma for one.So : one.So\n",
      "Lemma for please : please\n",
      "Lemma for take : take\n",
      "Lemma for another : another\n",
      "Lemma for hour : hour\n",
      "Lemma for in : in\n",
      "Lemma for thursdays : thursday\n",
      "Lemma for madame. : madame.\n",
      "Lemma for '' : ''\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for We : We\n",
      "Lemma for can : can\n",
      "Lemma for hear : hear\n",
      "Lemma for your : your\n",
      "Lemma for voice : voice\n",
      "Lemma for clearly : clearly\n",
      "Lemma for and : and\n",
      "Lemma for can : can\n",
      "Lemma for understand : understand\n",
      "Lemma for the : the\n",
      "Lemma for things : thing\n",
      "Lemma for you : you\n",
      "Lemma for teach : teach\n",
      "Lemma for . : .\n",
      "Lemma for Presentation : Presentation\n",
      "Lemma for slides : slide\n",
      "Lemma for also : also\n",
      "Lemma for good : good\n",
      "Lemma for source : source\n",
      "Lemma for to : to\n",
      "Lemma for refer : refer\n",
      "Lemma for . : .\n",
      "Lemma for lf : lf\n",
      "Lemma for you : you\n",
      "Lemma for can : can\n",
      "Lemma for do : do\n",
      "Lemma for more : more\n",
      "Lemma for example : example\n",
      "Lemma for questions : question\n",
      "Lemma for within : within\n",
      "Lemma for the : the\n",
      "Lemma for classroom : classroom\n",
      "Lemma for and : and\n",
      "Lemma for it : it\n",
      "Lemma for will : will\n",
      "Lemma for help : help\n",
      "Lemma for us : u\n",
      "Lemma for to : to\n",
      "Lemma for understand : understand\n",
      "Lemma for the : the\n",
      "Lemma for principles : principle\n",
      "Lemma for well : well\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for '' : ''\n",
      "Lemma for Lectures : Lectures\n",
      "Lemma for was : wa\n",
      "Lemma for well : well\n",
      "Lemma for structured : structured\n",
      "Lemma for and : and\n",
      "Lemma for well : well\n",
      "Lemma for organized : organized\n",
      "Lemma for . : .\n",
      "Lemma for It : It\n",
      "Lemma for was : wa\n",
      "Lemma for easy : easy\n",
      "Lemma for to : to\n",
      "Lemma for understand : understand\n",
      "Lemma for . : .\n",
      "Lemma for Lecture : Lecture\n",
      "Lemma for slides : slide\n",
      "Lemma for and : and\n",
      "Lemma for labs : lab\n",
      "Lemma for were : were\n",
      "Lemma for also : also\n",
      "Lemma for well : well\n",
      "Lemma for organized : organized\n",
      "Lemma for . : .\n",
      "Lemma for Lectures : Lectures\n",
      "Lemma for were : were\n",
      "Lemma for good : good\n",
      "Lemma for . : .\n",
      "Lemma for understandable : understandable\n",
      "Lemma for . : .\n",
      "Lemma for The : The\n",
      "Lemma for lecture : lecture\n",
      "Lemma for slides : slide\n",
      "Lemma for were : were\n",
      "Lemma for well : well\n",
      "Lemma for organized : organized\n",
      "Lemma for and : and\n",
      "Lemma for the : the\n",
      "Lemma for examples : example\n",
      "Lemma for done : done\n",
      "Lemma for in : in\n",
      "Lemma for the : the\n",
      "Lemma for class : class\n",
      "Lemma for helped : helped\n",
      "Lemma for a : a\n",
      "Lemma for lot : lot\n",
      "Lemma for to : to\n",
      "Lemma for learn : learn\n",
      "Lemma for this : this\n",
      "Lemma for new : new\n",
      "Lemma for language : language\n",
      "Lemma for and : and\n",
      "Lemma for also : also\n",
      "Lemma for the : the\n",
      "Lemma for principles : principle\n",
      "Lemma for of : of\n",
      "Lemma for OOP : OOP\n",
      "Lemma for . : .\n",
      "Lemma for Motivated : Motivated\n",
      "Lemma for to : to\n",
      "Lemma for well : well\n",
      "Lemma for . : .\n",
      "Lemma for Would : Would\n",
      "Lemma for have : have\n",
      "Lemma for been : been\n",
      "Lemma for better : better\n",
      "Lemma for if : if\n",
      "Lemma for we : we\n",
      "Lemma for discussed : discussed\n",
      "Lemma for more : more\n",
      "Lemma for about : about\n",
      "Lemma for the : the\n",
      "Lemma for solutions : solution\n",
      "Lemma for of : of\n",
      "Lemma for coding : coding\n",
      "Lemma for exercisers : exerciser\n",
      "Lemma for . : .\n",
      "Lemma for I : I\n",
      "Lemma for think : think\n",
      "Lemma for i : i\n",
      "Lemma for learned : learned\n",
      "Lemma for a : a\n",
      "Lemma for lot : lot\n",
      "Lemma for from : from\n",
      "Lemma for the : the\n",
      "Lemma for codes : code\n",
      "Lemma for you : you\n",
      "Lemma for write : write\n",
      "Lemma for in : in\n",
      "Lemma for the : the\n",
      "Lemma for board : board\n",
      "Lemma for . : .\n",
      "Lemma for When : When\n",
      "Lemma for i : i\n",
      "Lemma for compare : compare\n",
      "Lemma for my : my\n",
      "Lemma for codes : code\n",
      "Lemma for with : with\n",
      "Lemma for yours : yours\n",
      "Lemma for i : i\n",
      "Lemma for can : can\n",
      "Lemma for learn : learn\n",
      "Lemma for about : about\n",
      "Lemma for my : my\n",
      "Lemma for mistakes : mistake\n",
      "Lemma for and : and\n",
      "Lemma for good : good\n",
      "Lemma for coding : coding\n",
      "Lemma for practices : practice\n",
      "Lemma for that : that\n",
      "Lemma for i : i\n",
      "Lemma for should : should\n",
      "Lemma for follow : follow\n",
      "Lemma for . : .\n",
      "Lemma for There : There\n",
      "Lemma for fore : fore\n",
      "Lemma for i : i\n",
      "Lemma for think : think\n",
      "Lemma for it : it\n",
      "Lemma for would : would\n",
      "Lemma for be : be\n",
      "Lemma for great : great\n",
      "Lemma for if : if\n",
      "Lemma for we : we\n",
      "Lemma for can : can\n",
      "Lemma for discuss : discus\n",
      "Lemma for more : more\n",
      "Lemma for examples : example\n",
      "Lemma for in : in\n",
      "Lemma for the : the\n",
      "Lemma for class : class\n",
      "Lemma for . : .\n",
      "Lemma for madam : madam\n",
      "Lemma for explained : explained\n",
      "Lemma for the : the\n",
      "Lemma for oop : oop\n",
      "Lemma for concepts : concept\n",
      "Lemma for clearly : clearly\n",
      "Lemma for with : with\n",
      "Lemma for examples.lectures : examples.lectures\n",
      "Lemma for were : were\n",
      "Lemma for interesting.we : interesting.we\n",
      "Lemma for want : want\n",
      "Lemma for more : more\n",
      "Lemma for scenario : scenario\n",
      "Lemma for examples : example\n",
      "Lemma for and : and\n",
      "Lemma for answers : answer\n",
      "Lemma for with : with\n",
      "Lemma for explanations : explanation\n",
      "Lemma for in : in\n",
      "Lemma for future : future\n",
      "Lemma for . : .\n",
      "Lemma for I : I\n",
      "Lemma for satisfy : satisfy\n",
      "Lemma for about : about\n",
      "Lemma for first : first\n",
      "Lemma for 7 : 7\n",
      "Lemma for lectures : lecture\n",
      "Lemma for . : .\n",
      "Lemma for That : That\n",
      "Lemma for way : way\n",
      "Lemma for of : of\n",
      "Lemma for teaching : teaching\n",
      "Lemma for is : is\n",
      "Lemma for really : really\n",
      "Lemma for good : good\n",
      "Lemma for for : for\n",
      "Lemma for coming : coming\n",
      "Lemma for lectures : lecture\n",
      "Lemma for too : too\n",
      "Lemma for . : .\n",
      "Lemma for lectuers : lectuers\n",
      "Lemma for are : are\n",
      "Lemma for very : very\n",
      "Lemma for good : good\n",
      "Lemma for . : .\n",
      "Lemma for take : take\n",
      "Lemma for good : good\n",
      "Lemma for effort : effort\n",
      "Lemma for to : to\n",
      "Lemma for make : make\n",
      "Lemma for undersatand : undersatand\n",
      "Lemma for every : every\n",
      "Lemma for student : student\n",
      "Lemma for in : in\n",
      "Lemma for the : the\n",
      "Lemma for room : room\n",
      "Lemma for . : .\n",
      "Lemma for very : very\n",
      "Lemma for helpfull : helpfull\n",
      "Lemma for . : .\n",
      "Lemma for I : I\n",
      "Lemma for was : wa\n",
      "Lemma for able : able\n",
      "Lemma for to : to\n",
      "Lemma for obtain : obtain\n",
      "Lemma for a : a\n",
      "Lemma for clear : clear\n",
      "Lemma for picture : picture\n",
      "Lemma for about : about\n",
      "Lemma for OOP : OOP\n",
      "Lemma for and : and\n",
      "Lemma for its : it\n",
      "Lemma for concepts : concept\n",
      "Lemma for . : .\n",
      "Lemma for `` : ``\n",
      "Lemma for lecture : lecture\n",
      "Lemma for slides : slide\n",
      "Lemma for , : ,\n",
      "Lemma for explanations : explanation\n",
      "Lemma for were : were\n",
      "Lemma for very : very\n",
      "Lemma for clear : clear\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for it : it\n",
      "Lemma for & : &\n",
      "Lemma for # : #\n",
      "Lemma for 039 : 039\n",
      "Lemma for ; : ;\n",
      "Lemma for s : s\n",
      "Lemma for very : very\n",
      "Lemma for good : good\n",
      "Lemma for to : to\n",
      "Lemma for letting : letting\n",
      "Lemma for ask : ask\n",
      "Lemma for questions : question\n",
      "Lemma for and : and\n",
      "Lemma for explain : explain\n",
      "Lemma for again : again\n",
      "Lemma for with : with\n",
      "Lemma for suitable : suitable\n",
      "Lemma for examples : example\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for sometimes : sometimes\n",
      "Lemma for , : ,\n",
      "Lemma for some : some\n",
      "Lemma for codes : code\n",
      "Lemma for on : on\n",
      "Lemma for white : white\n",
      "Lemma for board : board\n",
      "Lemma for were : were\n",
      "Lemma for unclear : unclear\n",
      "Lemma for at : at\n",
      "Lemma for the : the\n",
      "Lemma for back : back\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for overall : overall\n",
      "Lemma for very : very\n",
      "Lemma for good : good\n",
      "Lemma for ! : !\n",
      "Lemma for ! : !\n",
      "Lemma for ! : !\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for '' : ''\n",
      "Lemma for The : The\n",
      "Lemma for lectures : lecture\n",
      "Lemma for were : were\n",
      "Lemma for good : good\n",
      "Lemma for and : and\n",
      "Lemma for clear : clear\n",
      "Lemma for . : .\n",
      "Lemma for And : And\n",
      "Lemma for they : they\n",
      "Lemma for weren : weren\n",
      "Lemma for & : &\n",
      "Lemma for # : #\n",
      "Lemma for 039 : 039\n",
      "Lemma for ; : ;\n",
      "Lemma for t : t\n",
      "Lemma for too : too\n",
      "Lemma for fast : fast\n",
      "Lemma for . : .\n",
      "Lemma for Writing : Writing\n",
      "Lemma for code : code\n",
      "Lemma for was : wa\n",
      "Lemma for somewhat : somewhat\n",
      "Lemma for confusing : confusing\n",
      "Lemma for because : because\n",
      "Lemma for I : I\n",
      "Lemma for didn : didn\n",
      "Lemma for & : &\n",
      "Lemma for # : #\n",
      "Lemma for 039 : 039\n",
      "Lemma for ; : ;\n",
      "Lemma for t : t\n",
      "Lemma for know : know\n",
      "Lemma for java : java\n",
      "Lemma for before : before\n",
      "Lemma for . : .\n",
      "Lemma for Actually : Actually\n",
      "Lemma for teaching : teaching\n",
      "Lemma for is : is\n",
      "Lemma for very : very\n",
      "Lemma for good : good\n",
      "Lemma for and : and\n",
      "Lemma for can : can\n",
      "Lemma for understand : understand\n",
      "Lemma for easily : easily\n",
      "Lemma for the : the\n",
      "Lemma for concepts : concept\n",
      "Lemma for by : by\n",
      "Lemma for examples : example\n",
      "Lemma for which : which\n",
      "Lemma for are : are\n",
      "Lemma for given : given\n",
      "Lemma for in : in\n",
      "Lemma for the : the\n",
      "Lemma for class.it : class.it\n",
      "Lemma for will : will\n",
      "Lemma for be : be\n",
      "Lemma for more : more\n",
      "Lemma for helpful : helpful\n",
      "Lemma for if : if\n",
      "Lemma for provide : provide\n",
      "Lemma for solved : solved\n",
      "Lemma for questions : question\n",
      "Lemma for as : a\n",
      "Lemma for well : well\n",
      "Lemma for ! : !\n",
      "Lemma for . : .\n",
      "Lemma for thankyou : thankyou\n"
     ]
    }
   ],
   "source": [
    "tokenization = nltk.word_tokenize(feedbackRawData)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} : {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Neural : Neural\n",
      "Lemma for network : network\n",
      "Lemma for models : model\n",
      "Lemma for have : have\n",
      "Lemma for shown : shown\n",
      "Lemma for their : their\n",
      "Lemma for promising : promising\n",
      "Lemma for opportunities : opportunity\n",
      "Lemma for for : for\n",
      "Lemma for multi-task : multi-task\n",
      "Lemma for learning : learning\n",
      "Lemma for , : ,\n",
      "Lemma for which : which\n",
      "Lemma for focus : focus\n",
      "Lemma for on : on\n",
      "Lemma for learning : learning\n",
      "Lemma for the : the\n",
      "Lemma for shared : shared\n",
      "Lemma for layers : layer\n",
      "Lemma for to : to\n",
      "Lemma for extract : extract\n",
      "Lemma for the : the\n",
      "Lemma for common : common\n",
      "Lemma for and : and\n",
      "Lemma for task-invariant : task-invariant\n",
      "Lemma for features : feature\n",
      "Lemma for . : .\n",
      "Lemma for However : However\n",
      "Lemma for , : ,\n",
      "Lemma for in : in\n",
      "Lemma for most : most\n",
      "Lemma for existing : existing\n",
      "Lemma for approaches : approach\n",
      "Lemma for , : ,\n",
      "Lemma for the : the\n",
      "Lemma for extracted : extracted\n",
      "Lemma for shared : shared\n",
      "Lemma for features : feature\n",
      "Lemma for are : are\n",
      "Lemma for prone : prone\n",
      "Lemma for to : to\n",
      "Lemma for be : be\n",
      "Lemma for contaminated : contaminated\n",
      "Lemma for by : by\n",
      "Lemma for task-specific : task-specific\n",
      "Lemma for features : feature\n",
      "Lemma for or : or\n",
      "Lemma for the : the\n",
      "Lemma for noise : noise\n",
      "Lemma for brought : brought\n",
      "Lemma for by : by\n",
      "Lemma for other : other\n",
      "Lemma for tasks : task\n",
      "Lemma for . : .\n",
      "Lemma for In : In\n",
      "Lemma for this : this\n",
      "Lemma for paper : paper\n",
      "Lemma for , : ,\n",
      "Lemma for we : we\n",
      "Lemma for propose : propose\n",
      "Lemma for an : an\n",
      "Lemma for adversarial : adversarial\n",
      "Lemma for multi-task : multi-task\n",
      "Lemma for learning : learning\n",
      "Lemma for framework : framework\n",
      "Lemma for , : ,\n",
      "Lemma for alleviating : alleviating\n",
      "Lemma for the : the\n",
      "Lemma for shared : shared\n",
      "Lemma for and : and\n",
      "Lemma for private : private\n",
      "Lemma for latent : latent\n",
      "Lemma for feature : feature\n",
      "Lemma for spaces : space\n",
      "Lemma for from : from\n",
      "Lemma for interfering : interfering\n",
      "Lemma for with : with\n",
      "Lemma for each : each\n",
      "Lemma for other : other\n",
      "Lemma for . : .\n",
      "Lemma for We : We\n",
      "Lemma for conduct : conduct\n",
      "Lemma for extensive : extensive\n",
      "Lemma for experiments : experiment\n",
      "Lemma for on : on\n",
      "Lemma for 16 : 16\n",
      "Lemma for different : different\n",
      "Lemma for text : text\n",
      "Lemma for classification : classification\n",
      "Lemma for tasks : task\n",
      "Lemma for , : ,\n",
      "Lemma for which : which\n",
      "Lemma for demonstrates : demonstrates\n",
      "Lemma for the : the\n",
      "Lemma for benefits : benefit\n",
      "Lemma for of : of\n",
      "Lemma for our : our\n",
      "Lemma for approach : approach\n",
      "Lemma for . : .\n",
      "Lemma for Besides : Besides\n",
      "Lemma for , : ,\n",
      "Lemma for we : we\n",
      "Lemma for show : show\n",
      "Lemma for that : that\n",
      "Lemma for the : the\n",
      "Lemma for shared : shared\n",
      "Lemma for knowledge : knowledge\n",
      "Lemma for learned : learned\n",
      "Lemma for by : by\n",
      "Lemma for our : our\n",
      "Lemma for proposed : proposed\n",
      "Lemma for model : model\n",
      "Lemma for can : can\n",
      "Lemma for be : be\n",
      "Lemma for regarded : regarded\n",
      "Lemma for as : a\n",
      "Lemma for off-the-shelf : off-the-shelf\n",
      "Lemma for knowledge : knowledge\n",
      "Lemma for and : and\n",
      "Lemma for easily : easily\n",
      "Lemma for transferred : transferred\n",
      "Lemma for to : to\n",
      "Lemma for new : new\n",
      "Lemma for tasks : task\n",
      "Lemma for . : .\n",
      "Lemma for Multi-task : Multi-task\n",
      "Lemma for learning : learning\n",
      "Lemma for is : is\n",
      "Lemma for an : an\n",
      "Lemma for effective : effective\n",
      "Lemma for approach : approach\n",
      "Lemma for to : to\n",
      "Lemma for improve : improve\n",
      "Lemma for the : the\n",
      "Lemma for performance : performance\n",
      "Lemma for of : of\n",
      "Lemma for a : a\n",
      "Lemma for single : single\n",
      "Lemma for task : task\n",
      "Lemma for with : with\n",
      "Lemma for the : the\n",
      "Lemma for help : help\n",
      "Lemma for of : of\n",
      "Lemma for other : other\n",
      "Lemma for related : related\n",
      "Lemma for tasks : task\n",
      "Lemma for . : .\n",
      "Lemma for Recently : Recently\n",
      "Lemma for , : ,\n",
      "Lemma for neural-based : neural-based\n",
      "Lemma for models : model\n",
      "Lemma for for : for\n",
      "Lemma for multi-task : multi-task\n",
      "Lemma for learning : learning\n",
      "Lemma for have : have\n",
      "Lemma for become : become\n",
      "Lemma for very : very\n",
      "Lemma for popular : popular\n",
      "Lemma for , : ,\n",
      "Lemma for ranging : ranging\n",
      "Lemma for from : from\n",
      "Lemma for computer : computer\n",
      "Lemma for vision : vision\n",
      "Lemma for ( : (\n",
      "Lemma for Misra : Misra\n",
      "Lemma for et : et\n",
      "Lemma for al. : al.\n",
      "Lemma for , : ,\n",
      "Lemma for 2016 : 2016\n",
      "Lemma for ; : ;\n",
      "Lemma for Zhang : Zhang\n",
      "Lemma for et : et\n",
      "Lemma for al. : al.\n",
      "Lemma for , : ,\n",
      "Lemma for 2014 : 2014\n",
      "Lemma for ) : )\n",
      "Lemma for to : to\n",
      "Lemma for natural : natural\n",
      "Lemma for language : language\n",
      "Lemma for processing : processing\n",
      "Lemma for ( : (\n",
      "Lemma for Collobert : Collobert\n",
      "Lemma for andWeston : andWeston\n",
      "Lemma for , : ,\n",
      "Lemma for 2008 : 2008\n",
      "Lemma for ; : ;\n",
      "Lemma for Luong : Luong\n",
      "Lemma for et : et\n",
      "Lemma for al. : al.\n",
      "Lemma for , : ,\n",
      "Lemma for 2015 : 2015\n",
      "Lemma for ) : )\n",
      "Lemma for , : ,\n",
      "Lemma for since : since\n",
      "Lemma for they : they\n",
      "Lemma for provide : provide\n",
      "Lemma for a : a\n",
      "Lemma for convenient : convenient\n",
      "Lemma for way : way\n",
      "Lemma for of : of\n",
      "Lemma for combining : combining\n",
      "Lemma for information : information\n",
      "Lemma for from : from\n",
      "Lemma for multiple : multiple\n",
      "Lemma for tasks : task\n",
      "Lemma for . : .\n",
      "Lemma for However : However\n",
      "Lemma for , : ,\n",
      "Lemma for most : most\n",
      "Lemma for existing : existing\n",
      "Lemma for work : work\n",
      "Lemma for on : on\n",
      "Lemma for multi-task : multi-task\n",
      "Lemma for learning : learning\n",
      "Lemma for ( : (\n",
      "Lemma for Liu : Liu\n",
      "Lemma for et : et\n",
      "Lemma for al. : al.\n",
      "Lemma for , : ,\n",
      "Lemma for 2016c : 2016c\n",
      "Lemma for , : ,\n",
      "Lemma for b : b\n",
      "Lemma for ) : )\n",
      "Lemma for attempts : attempt\n",
      "Lemma for to : to\n",
      "Lemma for divide : divide\n",
      "Lemma for the : the\n",
      "Lemma for features : feature\n",
      "Lemma for of : of\n",
      "Lemma for different : different\n",
      "Lemma for tasks : task\n",
      "Lemma for into : into\n",
      "Lemma for private : private\n",
      "Lemma for and : and\n",
      "Lemma for shared : shared\n",
      "Lemma for spaces : space\n",
      "Lemma for , : ,\n",
      "Lemma for merely : merely\n",
      "Lemma for based : based\n",
      "Lemma for on : on\n",
      "Lemma for whether : whether\n",
      "Lemma for parameters : parameter\n",
      "Lemma for of : of\n",
      "Lemma for some : some\n",
      "Lemma for components : component\n",
      "Lemma for should : should\n",
      "Lemma for be : be\n",
      "Lemma for shared : shared\n",
      "Lemma for . : .\n",
      "Lemma for As : As\n",
      "Lemma for shown : shown\n",
      "Lemma for in : in\n",
      "Lemma for Figure : Figure\n",
      "Lemma for 1- : 1-\n",
      "Lemma for ( : (\n",
      "Lemma for a : a\n",
      "Lemma for ) : )\n",
      "Lemma for , : ,\n",
      "Lemma for the : the\n",
      "Lemma for general : general\n",
      "Lemma for shared-private : shared-private\n",
      "Lemma for model : model\n",
      "Lemma for introduces : introduces\n",
      "Lemma for two : two\n",
      "Lemma for feature : feature\n",
      "Lemma for spaces : space\n",
      "Lemma for for : for\n",
      "Lemma for any : any\n",
      "Lemma for task : task\n",
      "Lemma for : : :\n",
      "Lemma for one : one\n",
      "Lemma for is : is\n",
      "Lemma for used : used\n",
      "Lemma for to : to\n",
      "Lemma for store : store\n",
      "Lemma for task-dependent : task-dependent\n",
      "Lemma for features : feature\n",
      "Lemma for , : ,\n",
      "Lemma for the : the\n",
      "Lemma for other : other\n",
      "Lemma for is : is\n",
      "Lemma for used : used\n",
      "Lemma for to : to\n",
      "Lemma for capture : capture\n",
      "Lemma for shared : shared\n",
      "Lemma for features : feature\n",
      "Lemma for . : .\n",
      "Lemma for The : The\n",
      "Lemma for major : major\n",
      "Lemma for limitation : limitation\n",
      "Lemma for of : of\n",
      "Lemma for this : this\n",
      "Lemma for framework : framework\n",
      "Lemma for is : is\n",
      "Lemma for that : that\n",
      "Lemma for the : the\n",
      "Lemma for shared : shared\n",
      "Lemma for feature : feature\n",
      "Lemma for space : space\n",
      "Lemma for could : could\n",
      "Lemma for contain : contain\n",
      "Lemma for some : some\n",
      "Lemma for unnecessary : unnecessary\n",
      "Lemma for task-specific : task-specific\n",
      "Lemma for features : feature\n",
      "Lemma for , : ,\n",
      "Lemma for while : while\n",
      "Lemma for some : some\n",
      "Lemma for sharable : sharable\n",
      "Lemma for features : feature\n",
      "Lemma for could : could\n",
      "Lemma for also : also\n",
      "Lemma for be : be\n",
      "Lemma for mixed : mixed\n",
      "Lemma for in : in\n",
      "Lemma for private : private\n",
      "Lemma for space : space\n",
      "Lemma for , : ,\n",
      "Lemma for suffering : suffering\n",
      "Lemma for from : from\n",
      "Lemma for feature : feature\n",
      "Lemma for redundancy : redundancy\n",
      "Lemma for . : .\n",
      "Lemma for Taking : Taking\n",
      "Lemma for the : the\n",
      "Lemma for following : following\n",
      "Lemma for two : two\n",
      "Lemma for sentences : sentence\n",
      "Lemma for as : a\n",
      "Lemma for examples : example\n",
      "Lemma for , : ,\n",
      "Lemma for which : which\n",
      "Lemma for are : are\n",
      "Lemma for extracted : extracted\n",
      "Lemma for from : from\n",
      "Lemma for two : two\n",
      "Lemma for different : different\n",
      "Lemma for sentiment : sentiment\n",
      "Lemma for classification : classification\n",
      "Lemma for tasks : task\n",
      "Lemma for : : :\n",
      "Lemma for Movie : Movie\n",
      "Lemma for reviews : review\n",
      "Lemma for and : and\n",
      "Lemma for Baby : Baby\n",
      "Lemma for products : product\n",
      "Lemma for reviews : review\n",
      "Lemma for . : .\n",
      "Lemma for The : The\n",
      "Lemma for infantile : infantile\n",
      "Lemma for cart : cart\n",
      "Lemma for is : is\n",
      "Lemma for simple : simple\n",
      "Lemma for and : and\n",
      "Lemma for easy : easy\n",
      "Lemma for to : to\n",
      "Lemma for use : use\n",
      "Lemma for . : .\n",
      "Lemma for This : This\n",
      "Lemma for kind : kind\n",
      "Lemma for of : of\n",
      "Lemma for humour : humour\n",
      "Lemma for is : is\n",
      "Lemma for infantile : infantile\n",
      "Lemma for and : and\n",
      "Lemma for boring : boring\n",
      "Lemma for . : .\n",
      "Lemma for The : The\n",
      "Lemma for word : word\n",
      "Lemma for �infantile� : �infantile�\n",
      "Lemma for indicates : indicates\n",
      "Lemma for negative : negative\n",
      "Lemma for sentiment : sentiment\n",
      "Lemma for in : in\n",
      "Lemma for Movie : Movie\n",
      "Lemma for task : task\n",
      "Lemma for while : while\n",
      "Lemma for it : it\n",
      "Lemma for is : is\n",
      "Lemma for neutral : neutral\n",
      "Lemma for in : in\n",
      "Lemma for Baby : Baby\n",
      "Lemma for task : task\n",
      "Lemma for . : .\n",
      "Lemma for However : However\n",
      "Lemma for , : ,\n",
      "Lemma for the : the\n",
      "Lemma for general : general\n",
      "Lemma for shared-private : shared-private\n",
      "Lemma for model : model\n",
      "Lemma for could : could\n",
      "Lemma for place : place\n",
      "Lemma for the : the\n",
      "Lemma for task-specific : task-specific\n",
      "Lemma for word : word\n",
      "Lemma for �infantile� : �infantile�\n",
      "Lemma for in : in\n",
      "Lemma for a : a\n",
      "Lemma for shared : shared\n",
      "Lemma for space : space\n",
      "Lemma for , : ,\n",
      "Lemma for leaving : leaving\n",
      "Lemma for potential : potential\n",
      "Lemma for hazards : hazard\n",
      "Lemma for for : for\n",
      "Lemma for other : other\n",
      "Lemma for tasks : task\n",
      "Lemma for . : .\n",
      "Lemma for Additionally : Additionally\n",
      "Lemma for , : ,\n",
      "Lemma for the : the\n",
      "Lemma for capacity : capacity\n",
      "Lemma for of : of\n",
      "Lemma for shared : shared\n",
      "Lemma for space : space\n",
      "Lemma for could : could\n",
      "Lemma for also : also\n",
      "Lemma for be : be\n",
      "Lemma for wasted : wasted\n",
      "Lemma for by : by\n",
      "Lemma for some : some\n",
      "Lemma for unnecessary : unnecessary\n",
      "Lemma for features : feature\n",
      "Lemma for . : .\n",
      "Lemma for To : To\n",
      "Lemma for address : address\n",
      "Lemma for this : this\n",
      "Lemma for problem : problem\n",
      "Lemma for , : ,\n",
      "Lemma for in : in\n",
      "Lemma for this : this\n",
      "Lemma for paper : paper\n",
      "Lemma for we : we\n",
      "Lemma for propose : propose\n",
      "Lemma for an : an\n",
      "Lemma for adversarial : adversarial\n",
      "Lemma for multi-task : multi-task\n",
      "Lemma for framework : framework\n",
      "Lemma for , : ,\n",
      "Lemma for in : in\n",
      "Lemma for which : which\n",
      "Lemma for the : the\n",
      "Lemma for shared : shared\n",
      "Lemma for and : and\n",
      "Lemma for private : private\n",
      "Lemma for feature : feature\n",
      "Lemma for spaces : space\n",
      "Lemma for are : are\n",
      "Lemma for in : in\n",
      "Lemma for herently : herently\n",
      "Lemma for disjoint : disjoint\n",
      "Lemma for by : by\n",
      "Lemma for introducing : introducing\n",
      "Lemma for orthogonality : orthogonality\n",
      "Lemma for constraints.Specifically : constraints.Specifically\n",
      "Lemma for , : ,\n",
      "Lemma for we : we\n",
      "Lemma for design : design\n",
      "Lemma for a : a\n",
      "Lemma for generic : generic\n",
      "Lemma for shared : shared\n",
      "Lemma for private : private\n",
      "Lemma for learning : learning\n",
      "Lemma for framework : framework\n",
      "Lemma for to : to\n",
      "Lemma for model : model\n",
      "Lemma for the : the\n",
      "Lemma for text : text\n",
      "Lemma for sequence : sequence\n",
      "Lemma for . : .\n"
     ]
    }
   ],
   "source": [
    "tokenization = nltk.word_tokenize(raw)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} : {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminds: remind\n",
      "me: me\n",
      "of: of\n",
      "Liberal: liber\n",
      "Immigration: immigr\n",
      "Fraudster: fraudster\n",
      "Monsef: monsef\n",
      "avoiding: avoid\n",
      "deportation: deport\n",
      "from: from\n",
      "Canada: canada\n",
      ".: .\n",
      "#cdnpoli: #cdnpoli\n",
      "#LPC: #lpc\n",
      "#CPCLDR: #cpcldr\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/ZOZOSe1CqQ: https://t.co/zozose1cqq\n",
      "#immigration: #immigr\n",
      "#integration: #integr\n",
      "#canada: #canada\n",
      "https://t.co/M5cKGyvV8F: https://t.co/m5ckgyvv8f\n",
      "We: We\n",
      "want: want\n",
      "controlled: control\n",
      "immigration: immigr\n",
      "that: that\n",
      "contributes: contribut\n",
      "positively: posit\n",
      "to: to\n",
      "the: the\n",
      "UK: UK\n",
      "economy: economi\n",
      ".: .\n",
      "Same: same\n",
      "as: as\n",
      "Australia: australia\n",
      "&: &\n",
      "Canada: canada\n",
      ".: .\n",
      "https://t.co/99mYliuOes: https://t.co/99myliuo\n",
      "Is: Is\n",
      "the: the\n",
      "new: new\n",
      "Manitoba: manitoba\n",
      "immigration: immigr\n",
      "fee: fee\n",
      "a: a\n",
      "head: head\n",
      "tax: tax\n",
      "?: ?\n",
      "https://t.co/LsG7C3vLe9: https://t.co/lsg7c3vle9\n",
      "Canada: canada\n",
      "immigration: immigr\n",
      "profit: profit\n",
      "influence: influenc\n",
      "modernistic: modernist\n",
      "delhi: delhi\n",
      "yet: yet\n",
      "abhinav: abhinav\n",
      ":: :\n",
      "XKofy: xkofi\n",
      "https://t.co/becgusY2i6: https://t.co/becgusy2i6\n",
      "Canada: canada\n",
      "Immigration: immigr\n",
      "Minister: minist\n",
      "to: to\n",
      "�: �\n",
      "�: �\n",
      "�: �\n",
      "Substantially: substanti\n",
      "Increase: increas\n",
      "Immigration: immigr\n",
      "Numbers: number\n",
      "https://t.co/nEFw30MRaa: https://t.co/nefw30mraa\n",
      "https://t.co/cyI867PZRV: https://t.co/cyi867pzrv\n",
      "M: M\n",
      "�: �\n",
      "�: �\n",
      "me: me\n",
      "les: le\n",
      "#USA: #usa\n",
      "=p: =p\n",
      "ays: ay\n",
      "d'immigration: d'immigr\n",
      "par: par\n",
      "excellence: excel\n",
      "CONTR: contr\n",
      "�: �\n",
      "�: �\n",
      "LE: LE\n",
      "RIGOUREUSEMENT: rigoureus\n",
      "l'immigration: l'immigr\n",
      "et: et\n",
      "acc: acc\n",
      "�: �\n",
      "�: �\n",
      "s: s\n",
      "�: �\n",
      "�: �\n",
      "la: la\n",
      "#GreenCARD: #greencard\n",
      "!: !\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/IHpVhW2BaG: https://t.co/ihpvhw2bag\n",
      "@Shawhelp: @shawhelp\n",
      "what: what\n",
      "changes: chang\n",
      "should: should\n",
      "be: be\n",
      "made: made\n",
      "to: to\n",
      "Canada's: canada'\n",
      "immigration: immigr\n",
      "laws: law\n",
      "due: due\n",
      "to: to\n",
      "the: the\n",
      "influx: influx\n",
      "of: of\n",
      "immigration: immigr\n",
      "and: and\n",
      "violence: violenc\n",
      "?: ?\n",
      "L: L\n",
      "�: �\n",
      "�: �\n",
      "immigration: immigr\n",
      "irr: irr\n",
      "�: �\n",
      "�: �\n",
      "guli: guli\n",
      "�: �\n",
      "�: �\n",
      "re: re\n",
      "au: au\n",
      "Canada: canada\n",
      "d: d\n",
      "�: �\n",
      "�: �\n",
      "cortiqu: cortiqu\n",
      "�: �\n",
      "�: �\n",
      "e: e\n",
      "en: en\n",
      "5: 5\n",
      "questions: question\n",
      "https://t.co/f4utO5A7ZF: https://t.co/f4uto5a7zf\n",
      "L'immigration: l'immigr\n",
      "irr: irr\n",
      "�: �\n",
      "�: �\n",
      "guli: guli\n",
      "�: �\n",
      "�: �\n",
      "re: re\n",
      "au: au\n",
      "Canada: canada\n",
      "d: d\n",
      "�: �\n",
      "�: �\n",
      "cortiqu: cortiqu\n",
      "�: �\n",
      "�: �\n",
      "e: e\n",
      "en: en\n",
      "5: 5\n",
      "questions: question\n",
      "-: -\n",
      "https://t.co/UiBsEZOqas: https://t.co/uibsezoqa\n",
      "https://t.co/j77dEvjoiX: https://t.co/j77devjoix\n",
      "https://t.co/XXDeIG7Dbu: https://t.co/xxdeig7dbu\n",
      "Will: will\n",
      "Media: media\n",
      "ask: ask\n",
      "the: the\n",
      "Liberals: liber\n",
      "if: if\n",
      "they: they\n",
      "actually: actual\n",
      "have: have\n",
      "a: a\n",
      "solid: solid\n",
      "plan: plan\n",
      "for: for\n",
      "Canada: canada\n",
      "_: _\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "�: �\n",
      "_: _\n",
      "?: ?\n",
      "?: ?\n",
      "From: from\n",
      "my: my\n",
      "view: view\n",
      "-: -\n",
      "-: -\n",
      "immigration: immigr\n",
      "out: out\n",
      "of: of\n",
      "C: C\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/YAgwmZ8ECp: https://t.co/yagwmz8ecp\n",
      "Dan: dan\n",
      "Murray: murray\n",
      "of: of\n",
      "�: �\n",
      "�: �\n",
      "Immigration: immigr\n",
      "Watch: watch\n",
      "Canada: canada\n",
      "is: is\n",
      "xenophobic: xenophob\n",
      "racist: racist\n",
      "fear-mongering: fear-mong\n",
      "liar: liar\n",
      "#racism: #racism\n",
      "#canada: #canada\n",
      "#cdnpoli: #cdnpoli\n",
      "#hatecrime: #hatecrim\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/kwZ3csvYxM: https://t.co/kwz3csvyxm\n",
      "Le: Le\n",
      "Canada: canada\n",
      "lance: lanc\n",
      "une: une\n",
      "vaste: vast\n",
      "campagne: campagn\n",
      "d'immigration: d'immigr\n",
      "pour: pour\n",
      "faire: fair\n",
      "face: face\n",
      "�: �\n",
      "�: �\n",
      "son: son\n",
      "besoin: besoin\n",
      "de: de\n",
      "main: main\n",
      "d: d\n",
      "': '\n",
      "�: �\n",
      "�: �\n",
      "uvre: uvr\n",
      "https://t.co/kXdfMGTZzN: https://t.co/kxdfmgtzzn\n",
      "L: L\n",
      "�: �\n",
      "�: �\n",
      "#immigration: #immigr\n",
      "irr: irr\n",
      "�: �\n",
      "�: �\n",
      "guli: guli\n",
      "�: �\n",
      "�: �\n",
      "re: re\n",
      "au: au\n",
      "#Canada: #canada\n",
      "d: d\n",
      "�: �\n",
      "�: �\n",
      "cortiqu: cortiqu\n",
      "�: �\n",
      "�: �\n",
      "e: e\n",
      "en: en\n",
      "5: 5\n",
      "�: �\n",
      "�: �\n",
      "questions: question\n",
      "https://t.co/s3hu1OKKIG: https://t.co/s3hu1okkig\n",
      "@Canadidly: @canadidli\n",
      "I've: i'v\n",
      "read: read\n",
      "the: the\n",
      "Immigration: immigr\n",
      "laws: law\n",
      "of: of\n",
      "Canada: canada\n",
      "much: much\n",
      "stricter: stricter\n",
      "than: than\n",
      "the: the\n",
      "US: US\n",
      "Canada: canada\n",
      "Immigration: immigr\n",
      "Website: websit\n",
      "Traffic: traffic\n",
      "Surges: surg\n",
      "And: and\n",
      "Crashes: crash\n",
      "In: In\n",
      "Wake: wake\n",
      "Of: Of\n",
      "Trump: trump\n",
      "#fasttraffic: #fasttraff\n",
      ",: ,\n",
      "#sitetraffic: #sitetraff\n",
      ",: ,\n",
      "#website: #websit\n",
      ",: ,\n",
      "#traffic: #traffic\n",
      "https://t.co/zRlJ26jnkC: https://t.co/zrlj26jnkc\n",
      "Mr: Mr\n",
      "Know-all: know-al\n",
      "of: of\n",
      "Canada: canada\n",
      "Immigration: immigr\n",
      "https://t.co/wTQK4QDiKI: https://t.co/wtqk4qdiki\n",
      "Move: move\n",
      "to: to\n",
      "Canada: canada\n",
      "@LadyMadonna___: @ladymadonna___\n",
      "Oh: Oh\n",
      ",: ,\n",
      "immigration: immigr\n",
      "rules: rule\n",
      ",: ,\n",
      "you: you\n",
      "can't: can't\n",
      "...: ...\n",
      "https://t.co/5LIEVHO7A4: https://t.co/5lievho7a4\n",
      "#OnThisDay: #onthisday\n",
      "Annette: annett\n",
      "Toft: toft\n",
      "becomes: becom\n",
      "Canada's: canada'\n",
      "2: 2\n",
      "millionth: millionth\n",
      "immigrant: immigr\n",
      "since: sinc\n",
      "1945: 1945\n",
      ".: .\n",
      "Do: Do\n",
      "you: you\n",
      "know: know\n",
      "your: your\n",
      "family's: family'\n",
      "immigration: immigr\n",
      "st: st\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/UvRuw8eR1b: https://t.co/uvruw8er1b\n",
      ".: .\n",
      "@TheEconomist: @theeconomist\n",
      "profiles: profil\n",
      "Canada's: canada'\n",
      "open: open\n",
      "immigration: immigr\n",
      "policies: polici\n",
      "&: &\n",
      "how: how\n",
      "they: they\n",
      "contribute: contribut\n",
      "to: to\n",
      "our: our\n",
      "economic: econom\n",
      "success: success\n",
      ":: :\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/4K84EE8Y63: https://t.co/4k84ee8y63\n",
      "Hundreds: hundr\n",
      "may: may\n",
      "lose: lose\n",
      "Canadian: canadian\n",
      "citizenship: citizenship\n",
      ",: ,\n",
      "resident: resid\n",
      "status: statu\n",
      "because: becaus\n",
      "of: of\n",
      "one: one\n",
      "corrupt: corrupt\n",
      "immigration: immigr\n",
      "consultant: consult\n",
      "https://t.co/x2IfO0EXI2: https://t.co/x2ifo0exi2\n",
      "Immigration: immigr\n",
      "for: for\n",
      "canada: canada\n",
      "without: without\n",
      "india: india\n",
      ":: :\n",
      "an: an\n",
      "compassionate: compassion\n",
      "handle: handl\n",
      ":: :\n",
      "deyFy: deyfi\n",
      "\": \"\n",
      "#Jamaican: #jamaican\n",
      "#immigrants: #immigr\n",
      "#Canada: #canada\n",
      "https://t.co/vcmfYGadR5: https://t.co/vcmfygadr5\n",
      "#statistics: #statist\n",
      "#immigration: #immigr\n",
      "\": \"\n",
      "Mexican: mexican\n",
      "visa: visa\n",
      "lift: lift\n",
      "expected: expect\n",
      "to: to\n",
      "cost: cost\n",
      "Canada: canada\n",
      "$: $\n",
      "262M: 262m\n",
      "over: over\n",
      "a: a\n",
      "decade: decad\n",
      "https://t.co/9i72fRhtij: https://t.co/9i72frhtij\n",
      "Are: are\n",
      "people: peopl\n",
      "still: still\n",
      "moving: move\n",
      "to: to\n",
      "#Canada: #canada\n",
      "?: ?\n",
      "?: ?\n",
      "?: ?\n",
      "Oh: Oh\n",
      "that's: that'\n",
      "right: right\n",
      ",: ,\n",
      "they: they\n",
      "have: have\n",
      "real: real\n",
      "immigration: immigr\n",
      "laws: law\n",
      "and: and\n",
      "it's: it'\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/0C5OBfmxLG: https://t.co/0c5obfmxlg\n",
      "Here: here\n",
      "are: are\n",
      "more: more\n",
      "details: detail\n",
      "on: on\n",
      "the: the\n",
      "Richmond: richmond\n",
      ",: ,\n",
      "B: B\n",
      ".: .\n",
      "C: C\n",
      ".: .\n",
      "Immigration: immigr\n",
      "Consultant: consult\n",
      "Sunny: sunni\n",
      "Wang: wang\n",
      "who: who\n",
      "was: wa\n",
      "sentenced: sentenc\n",
      "to: to\n",
      "7: 7\n",
      "years: year\n",
      "in: in\n",
      "...: ...\n",
      "https://t.co/YXH5W53srO: https://t.co/yxh5w53sro\n",
      "I: I\n",
      "added: ad\n",
      "a: a\n",
      "video: video\n",
      "to: to\n",
      "a: a\n",
      "@YouTube: @youtub\n",
      "playlist: playlist\n",
      "https://t.co/CnEyWN40x3: https://t.co/cneywn40x3\n",
      "Funny: funni\n",
      "Talking: talk\n",
      "of: of\n",
      "Haryanavi: haryanavi\n",
      "Jat: jat\n",
      "with: with\n",
      "Canada: canada\n",
      "Immigration: immigr\n",
      "Girl: girl\n",
      "Agent: agent\n",
      "Mexicans: mexican\n",
      "Can: can\n",
      "Now: now\n",
      "Travel: travel\n",
      "Visa-Free: visa-fre\n",
      "To: To\n",
      "Canada: canada\n",
      "https://t.co/Ec3XHORO2s: https://t.co/ec3xhoro2\n",
      "https://t.co/RQRr5nebcG: https://t.co/rqrr5nebcg\n",
      "L: L\n",
      "�: �\n",
      "�: �\n",
      "immigration: immigr\n",
      "irr: irr\n",
      "�: �\n",
      "�: �\n",
      "guli: guli\n",
      "�: �\n",
      "�: �\n",
      "re: re\n",
      "au: au\n",
      "Canada: canada\n",
      "d: d\n",
      "�: �\n",
      "�: �\n",
      "cortiqu: cortiqu\n",
      "�: �\n",
      "�: �\n",
      "e: e\n",
      "en: en\n",
      "5: 5\n",
      "�: �\n",
      "�: �\n",
      "questions: question\n",
      "https://t.co/DkpuKyWmaK: https://t.co/dkpukywmak\n",
      "@SweetnessShawnB: @sweetnessshawnb\n",
      "Hes: he\n",
      "the: the\n",
      "POS: po\n",
      "that: that\n",
      "ramped: ramp\n",
      "up: up\n",
      "immigration: immigr\n",
      "for: for\n",
      "Canada: canada\n",
      ",: ,\n",
      "among: among\n",
      "other: other\n",
      "globalist: globalist\n",
      "policies: polici\n",
      ".: .\n",
      "Canada: canada\n",
      "lifted: lift\n",
      "visa: visa\n",
      "requirements: requir\n",
      "to: to\n",
      "Mexico: mexico\n",
      "as: as\n",
      "of: of\n",
      "Dec: dec\n",
      "1: 1\n",
      ",: ,\n",
      "2016: 2016\n",
      ".: .\n",
      "Thoughts: thought\n",
      "?: ?\n",
      "#visa: #visa\n",
      "#immigration: #immigr\n",
      "@HuffingtonPost: @huffingtonpost\n",
      "people: peopl\n",
      "Keep: keep\n",
      "praising: prais\n",
      "Canada: canada\n",
      "and: and\n",
      "Canada: canada\n",
      "has: ha\n",
      "way: way\n",
      "stricter: stricter\n",
      "immigration: immigr\n",
      "laws: law\n",
      "then: then\n",
      "us: us\n",
      "they: they\n",
      "willl: willl\n",
      "boot: boot\n",
      "your: your\n",
      "liberal: liber\n",
      "American: american\n",
      "ass: ass\n"
     ]
    }
   ],
   "source": [
    "twitter_words= twitterTokens\n",
    "ps =PorterStemmer()\n",
    "for w in twitter_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(w + \": \" +rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural: neural\n",
      "network: network\n",
      "models: model\n",
      "have: have\n",
      "shown: shown\n",
      "their: their\n",
      "promising: promis\n",
      "opportunities: opportun\n",
      "for: for\n",
      "multi-task: multi-task\n",
      "learning: learn\n",
      ",: ,\n",
      "which: which\n",
      "focus: focu\n",
      "on: on\n",
      "learning: learn\n",
      "the: the\n",
      "shared: share\n",
      "layers: layer\n",
      "to: to\n",
      "extract: extract\n",
      "the: the\n",
      "common: common\n",
      "and: and\n",
      "task-invariant: task-invari\n",
      "features: featur\n",
      ".: .\n",
      "However: howev\n",
      ",: ,\n",
      "in: in\n",
      "most: most\n",
      "existing: exist\n",
      "approaches: approach\n",
      ",: ,\n",
      "the: the\n",
      "extracted: extract\n",
      "shared: share\n",
      "features: featur\n",
      "are: are\n",
      "prone: prone\n",
      "to: to\n",
      "be: be\n",
      "contaminated: contamin\n",
      "by: by\n",
      "task-specific: task-specif\n",
      "features: featur\n",
      "or: or\n",
      "the: the\n",
      "noise: nois\n",
      "brought: brought\n",
      "by: by\n",
      "other: other\n",
      "tasks: task\n",
      ".: .\n",
      "In: In\n",
      "this: thi\n",
      "paper: paper\n",
      ",: ,\n",
      "we: we\n",
      "propose: propos\n",
      "an: an\n",
      "adversarial: adversari\n",
      "multi-task: multi-task\n",
      "learning: learn\n",
      "framework: framework\n",
      ",: ,\n",
      "alleviating: allevi\n",
      "the: the\n",
      "shared: share\n",
      "and: and\n",
      "private: privat\n",
      "latent: latent\n",
      "feature: featur\n",
      "spaces: space\n",
      "from: from\n",
      "interfering: interf\n",
      "with: with\n",
      "each: each\n",
      "other: other\n",
      ".: .\n",
      "We: We\n",
      "conduct: conduct\n",
      "extensive: extens\n",
      "experiments: experi\n",
      "on: on\n",
      "16: 16\n",
      "different: differ\n",
      "text: text\n",
      "classification: classif\n",
      "tasks: task\n",
      ",: ,\n",
      "which: which\n",
      "demonstrates: demonstr\n",
      "the: the\n",
      "benefits: benefit\n",
      "of: of\n",
      "our: our\n",
      "approach: approach\n",
      ".: .\n",
      "Besides: besid\n",
      ",: ,\n",
      "we: we\n",
      "show: show\n",
      "that: that\n",
      "the: the\n",
      "shared: share\n",
      "knowledge: knowledg\n",
      "learned: learn\n",
      "by: by\n",
      "our: our\n",
      "proposed: propos\n",
      "model: model\n",
      "can: can\n",
      "be: be\n",
      "regarded: regard\n",
      "as: as\n",
      "off-the-shelf: off-the-shelf\n",
      "knowledge: knowledg\n",
      "and: and\n",
      "easily: easili\n",
      "transferred: transfer\n",
      "to: to\n",
      "new: new\n",
      "tasks: task\n",
      ".: .\n",
      "Multi-task: multi-task\n",
      "learning: learn\n",
      "is: is\n",
      "an: an\n",
      "effective: effect\n",
      "approach: approach\n",
      "to: to\n",
      "improve: improv\n",
      "the: the\n",
      "performance: perform\n",
      "of: of\n",
      "a: a\n",
      "single: singl\n",
      "task: task\n",
      "with: with\n",
      "the: the\n",
      "help: help\n",
      "of: of\n",
      "other: other\n",
      "related: relat\n",
      "tasks: task\n",
      ".: .\n",
      "Recently: recent\n",
      ",: ,\n",
      "neural-based: neural-bas\n",
      "models: model\n",
      "for: for\n",
      "multi-task: multi-task\n",
      "learning: learn\n",
      "have: have\n",
      "become: becom\n",
      "very: veri\n",
      "popular: popular\n",
      ",: ,\n",
      "ranging: rang\n",
      "from: from\n",
      "computer: comput\n",
      "vision: vision\n",
      "(: (\n",
      "Misra: misra\n",
      "et: et\n",
      "al.: al.\n",
      ",: ,\n",
      "2016: 2016\n",
      ";: ;\n",
      "Zhang: zhang\n",
      "et: et\n",
      "al.: al.\n",
      ",: ,\n",
      "2014: 2014\n",
      "): )\n",
      "to: to\n",
      "natural: natur\n",
      "language: languag\n",
      "processing: process\n",
      "(: (\n",
      "Collobert: collobert\n",
      "andWeston: andweston\n",
      ",: ,\n",
      "2008: 2008\n",
      ";: ;\n",
      "Luong: luong\n",
      "et: et\n",
      "al.: al.\n",
      ",: ,\n",
      "2015: 2015\n",
      "): )\n",
      ",: ,\n",
      "since: sinc\n",
      "they: they\n",
      "provide: provid\n",
      "a: a\n",
      "convenient: conveni\n",
      "way: way\n",
      "of: of\n",
      "combining: combin\n",
      "information: inform\n",
      "from: from\n",
      "multiple: multipl\n",
      "tasks: task\n",
      ".: .\n",
      "However: howev\n",
      ",: ,\n",
      "most: most\n",
      "existing: exist\n",
      "work: work\n",
      "on: on\n",
      "multi-task: multi-task\n",
      "learning: learn\n",
      "(: (\n",
      "Liu: liu\n",
      "et: et\n",
      "al.: al.\n",
      ",: ,\n",
      "2016c: 2016c\n",
      ",: ,\n",
      "b: b\n",
      "): )\n",
      "attempts: attempt\n",
      "to: to\n",
      "divide: divid\n",
      "the: the\n",
      "features: featur\n",
      "of: of\n",
      "different: differ\n",
      "tasks: task\n",
      "into: into\n",
      "private: privat\n",
      "and: and\n",
      "shared: share\n",
      "spaces: space\n",
      ",: ,\n",
      "merely: mere\n",
      "based: base\n",
      "on: on\n",
      "whether: whether\n",
      "parameters: paramet\n",
      "of: of\n",
      "some: some\n",
      "components: compon\n",
      "should: should\n",
      "be: be\n",
      "shared: share\n",
      ".: .\n",
      "As: As\n",
      "shown: shown\n",
      "in: in\n",
      "Figure: figur\n",
      "1-: 1-\n",
      "(: (\n",
      "a: a\n",
      "): )\n",
      ",: ,\n",
      "the: the\n",
      "general: gener\n",
      "shared-private: shared-priv\n",
      "model: model\n",
      "introduces: introduc\n",
      "two: two\n",
      "feature: featur\n",
      "spaces: space\n",
      "for: for\n",
      "any: ani\n",
      "task: task\n",
      ":: :\n",
      "one: one\n",
      "is: is\n",
      "used: use\n",
      "to: to\n",
      "store: store\n",
      "task-dependent: task-depend\n",
      "features: featur\n",
      ",: ,\n",
      "the: the\n",
      "other: other\n",
      "is: is\n",
      "used: use\n",
      "to: to\n",
      "capture: captur\n",
      "shared: share\n",
      "features: featur\n",
      ".: .\n",
      "The: the\n",
      "major: major\n",
      "limitation: limit\n",
      "of: of\n",
      "this: thi\n",
      "framework: framework\n",
      "is: is\n",
      "that: that\n",
      "the: the\n",
      "shared: share\n",
      "feature: featur\n",
      "space: space\n",
      "could: could\n",
      "contain: contain\n",
      "some: some\n",
      "unnecessary: unnecessari\n",
      "task-specific: task-specif\n",
      "features: featur\n",
      ",: ,\n",
      "while: while\n",
      "some: some\n",
      "sharable: sharabl\n",
      "features: featur\n",
      "could: could\n",
      "also: also\n",
      "be: be\n",
      "mixed: mix\n",
      "in: in\n",
      "private: privat\n",
      "space: space\n",
      ",: ,\n",
      "suffering: suffer\n",
      "from: from\n",
      "feature: featur\n",
      "redundancy: redund\n",
      ".: .\n",
      "Taking: take\n",
      "the: the\n",
      "following: follow\n",
      "two: two\n",
      "sentences: sentenc\n",
      "as: as\n",
      "examples: exampl\n",
      ",: ,\n",
      "which: which\n",
      "are: are\n",
      "extracted: extract\n",
      "from: from\n",
      "two: two\n",
      "different: differ\n",
      "sentiment: sentiment\n",
      "classification: classif\n",
      "tasks: task\n",
      ":: :\n",
      "Movie: movi\n",
      "reviews: review\n",
      "and: and\n",
      "Baby: babi\n",
      "products: product\n",
      "reviews: review\n",
      ".: .\n",
      "The: the\n",
      "infantile: infantil\n",
      "cart: cart\n",
      "is: is\n",
      "simple: simpl\n",
      "and: and\n",
      "easy: easi\n",
      "to: to\n",
      "use: use\n",
      ".: .\n",
      "This: thi\n",
      "kind: kind\n",
      "of: of\n",
      "humour: humour\n",
      "is: is\n",
      "infantile: infantil\n",
      "and: and\n",
      "boring: bore\n",
      ".: .\n",
      "The: the\n",
      "word: word\n",
      "�infantile�: �infantile�\n",
      "indicates: indic\n",
      "negative: neg\n",
      "sentiment: sentiment\n",
      "in: in\n",
      "Movie: movi\n",
      "task: task\n",
      "while: while\n",
      "it: it\n",
      "is: is\n",
      "neutral: neutral\n",
      "in: in\n",
      "Baby: babi\n",
      "task: task\n",
      ".: .\n",
      "However: howev\n",
      ",: ,\n",
      "the: the\n",
      "general: gener\n",
      "shared-private: shared-priv\n",
      "model: model\n",
      "could: could\n",
      "place: place\n",
      "the: the\n",
      "task-specific: task-specif\n",
      "word: word\n",
      "�infantile�: �infantile�\n",
      "in: in\n",
      "a: a\n",
      "shared: share\n",
      "space: space\n",
      ",: ,\n",
      "leaving: leav\n",
      "potential: potenti\n",
      "hazards: hazard\n",
      "for: for\n",
      "other: other\n",
      "tasks: task\n",
      ".: .\n",
      "Additionally: addit\n",
      ",: ,\n",
      "the: the\n",
      "capacity: capac\n",
      "of: of\n",
      "shared: share\n",
      "space: space\n",
      "could: could\n",
      "also: also\n",
      "be: be\n",
      "wasted: wast\n",
      "by: by\n",
      "some: some\n",
      "unnecessary: unnecessari\n",
      "features: featur\n",
      ".: .\n",
      "To: To\n",
      "address: address\n",
      "this: thi\n",
      "problem: problem\n",
      ",: ,\n",
      "in: in\n",
      "this: thi\n",
      "paper: paper\n",
      "we: we\n",
      "propose: propos\n",
      "an: an\n",
      "adversarial: adversari\n",
      "multi-task: multi-task\n",
      "framework: framework\n",
      ",: ,\n",
      "in: in\n",
      "which: which\n",
      "the: the\n",
      "shared: share\n",
      "and: and\n",
      "private: privat\n",
      "feature: featur\n",
      "spaces: space\n",
      "are: are\n",
      "in: in\n",
      "herently: herent\n",
      "disjoint: disjoint\n",
      "by: by\n",
      "introducing: introduc\n",
      "orthogonality: orthogon\n",
      "constraints.Specifically: constraints.specif\n",
      ",: ,\n",
      "we: we\n",
      "design: design\n",
      "a: a\n",
      "generic: gener\n",
      "shared: share\n",
      "private: privat\n",
      "learning: learn\n",
      "framework: framework\n",
      "to: to\n",
      "model: model\n",
      "the: the\n",
      "text: text\n",
      "sequence: sequenc\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "research_words= researchTokens\n",
    "ps =PorterStemmer()\n",
    "for w in research_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(w + \": \" +rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honestly: honestli\n",
      "last: last\n",
      "seven: seven\n",
      "lectures: lectur\n",
      "are: are\n",
      "good: good\n",
      ".: .\n",
      "Lectures: lectur\n",
      "are: are\n",
      "understandable: understand\n",
      ".: .\n",
      "Lecture: lectur\n",
      "slides: slide\n",
      "are: are\n",
      "very: veri\n",
      "useful: use\n",
      "to: to\n",
      "self-study: self-studi\n",
      "also: also\n",
      ".: .\n",
      "The: the\n",
      "given: given\n",
      "opportunity: opportun\n",
      "to: to\n",
      "ask: ask\n",
      "questions: question\n",
      "from: from\n",
      "the: the\n",
      "lecturer: lectur\n",
      "is: is\n",
      "appreciative: appreci\n",
      ".: .\n",
      "``: ``\n",
      "Good: good\n",
      ":: :\n",
      "): )\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "please: pleas\n",
      "do: do\n",
      "recap: recap\n",
      "at: at\n",
      "class: class\n",
      "starting: start\n",
      "it: it\n",
      "&: &\n",
      "#: #\n",
      "039: 039\n",
      ";: ;\n",
      "s: s\n",
      "better: better\n",
      "for: for\n",
      "us: us\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "sometimes: sometim\n",
      "teaching: teach\n",
      "speed: speed\n",
      "is: is\n",
      "very: veri\n",
      "high: high\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "Thanks: thank\n",
      "!: !\n",
      ":: :\n",
      "): )\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "``: ``\n",
      "The: the\n",
      "lectures: lectur\n",
      "are: are\n",
      "good..but: good..but\n",
      "a: a\n",
      "bit: bit\n",
      "speed.A: speed.a\n",
      "in: in\n",
      "class: class\n",
      "working: work\n",
      "activity: activ\n",
      "is: is\n",
      "a: a\n",
      "must: must\n",
      "one.So: one.so\n",
      "please: pleas\n",
      "take: take\n",
      "another: anoth\n",
      "hour: hour\n",
      "in: in\n",
      "thursdays: thursday\n",
      "madame.: madame.\n",
      "'': ''\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "We: We\n",
      "can: can\n",
      "hear: hear\n",
      "your: your\n",
      "voice: voic\n",
      "clearly: clearli\n",
      "and: and\n",
      "can: can\n",
      "understand: understand\n",
      "the: the\n",
      "things: thing\n",
      "you: you\n",
      "teach: teach\n",
      ".: .\n",
      "Presentation: present\n",
      "slides: slide\n",
      "also: also\n",
      "good: good\n",
      "source: sourc\n",
      "to: to\n",
      "refer: refer\n",
      ".: .\n",
      "lf: lf\n",
      "you: you\n",
      "can: can\n",
      "do: do\n",
      "more: more\n",
      "example: exampl\n",
      "questions: question\n",
      "within: within\n",
      "the: the\n",
      "classroom: classroom\n",
      "and: and\n",
      "it: it\n",
      "will: will\n",
      "help: help\n",
      "us: us\n",
      "to: to\n",
      "understand: understand\n",
      "the: the\n",
      "principles: principl\n",
      "well: well\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "'': ''\n",
      "Lectures: lectur\n",
      "was: wa\n",
      "well: well\n",
      "structured: structur\n",
      "and: and\n",
      "well: well\n",
      "organized: organ\n",
      ".: .\n",
      "It: It\n",
      "was: wa\n",
      "easy: easi\n",
      "to: to\n",
      "understand: understand\n",
      ".: .\n",
      "Lecture: lectur\n",
      "slides: slide\n",
      "and: and\n",
      "labs: lab\n",
      "were: were\n",
      "also: also\n",
      "well: well\n",
      "organized: organ\n",
      ".: .\n",
      "Lectures: lectur\n",
      "were: were\n",
      "good: good\n",
      ".: .\n",
      "understandable: understand\n",
      ".: .\n",
      "The: the\n",
      "lecture: lectur\n",
      "slides: slide\n",
      "were: were\n",
      "well: well\n",
      "organized: organ\n",
      "and: and\n",
      "the: the\n",
      "examples: exampl\n",
      "done: done\n",
      "in: in\n",
      "the: the\n",
      "class: class\n",
      "helped: help\n",
      "a: a\n",
      "lot: lot\n",
      "to: to\n",
      "learn: learn\n",
      "this: thi\n",
      "new: new\n",
      "language: languag\n",
      "and: and\n",
      "also: also\n",
      "the: the\n",
      "principles: principl\n",
      "of: of\n",
      "OOP: oop\n",
      ".: .\n",
      "Motivated: motiv\n",
      "to: to\n",
      "well: well\n",
      ".: .\n",
      "Would: would\n",
      "have: have\n",
      "been: been\n",
      "better: better\n",
      "if: if\n",
      "we: we\n",
      "discussed: discuss\n",
      "more: more\n",
      "about: about\n",
      "the: the\n",
      "solutions: solut\n",
      "of: of\n",
      "coding: code\n",
      "exercisers: exercis\n",
      ".: .\n",
      "I: I\n",
      "think: think\n",
      "i: i\n",
      "learned: learn\n",
      "a: a\n",
      "lot: lot\n",
      "from: from\n",
      "the: the\n",
      "codes: code\n",
      "you: you\n",
      "write: write\n",
      "in: in\n",
      "the: the\n",
      "board: board\n",
      ".: .\n",
      "When: when\n",
      "i: i\n",
      "compare: compar\n",
      "my: my\n",
      "codes: code\n",
      "with: with\n",
      "yours: your\n",
      "i: i\n",
      "can: can\n",
      "learn: learn\n",
      "about: about\n",
      "my: my\n",
      "mistakes: mistak\n",
      "and: and\n",
      "good: good\n",
      "coding: code\n",
      "practices: practic\n",
      "that: that\n",
      "i: i\n",
      "should: should\n",
      "follow: follow\n",
      ".: .\n",
      "There: there\n",
      "fore: fore\n",
      "i: i\n",
      "think: think\n",
      "it: it\n",
      "would: would\n",
      "be: be\n",
      "great: great\n",
      "if: if\n",
      "we: we\n",
      "can: can\n",
      "discuss: discuss\n",
      "more: more\n",
      "examples: exampl\n",
      "in: in\n",
      "the: the\n",
      "class: class\n",
      ".: .\n",
      "madam: madam\n",
      "explained: explain\n",
      "the: the\n",
      "oop: oop\n",
      "concepts: concept\n",
      "clearly: clearli\n",
      "with: with\n",
      "examples.lectures: examples.lectur\n",
      "were: were\n",
      "interesting.we: interesting.w\n",
      "want: want\n",
      "more: more\n",
      "scenario: scenario\n",
      "examples: exampl\n",
      "and: and\n",
      "answers: answer\n",
      "with: with\n",
      "explanations: explan\n",
      "in: in\n",
      "future: futur\n",
      ".: .\n",
      "I: I\n",
      "satisfy: satisfi\n",
      "about: about\n",
      "first: first\n",
      "7: 7\n",
      "lectures: lectur\n",
      ".: .\n",
      "That: that\n",
      "way: way\n",
      "of: of\n",
      "teaching: teach\n",
      "is: is\n",
      "really: realli\n",
      "good: good\n",
      "for: for\n",
      "coming: come\n",
      "lectures: lectur\n",
      "too: too\n",
      ".: .\n",
      "lectuers: lectuer\n",
      "are: are\n",
      "very: veri\n",
      "good: good\n",
      ".: .\n",
      "take: take\n",
      "good: good\n",
      "effort: effort\n",
      "to: to\n",
      "make: make\n",
      "undersatand: undersatand\n",
      "every: everi\n",
      "student: student\n",
      "in: in\n",
      "the: the\n",
      "room: room\n",
      ".: .\n",
      "very: veri\n",
      "helpfull: helpful\n",
      ".: .\n",
      "I: I\n",
      "was: wa\n",
      "able: abl\n",
      "to: to\n",
      "obtain: obtain\n",
      "a: a\n",
      "clear: clear\n",
      "picture: pictur\n",
      "about: about\n",
      "OOP: oop\n",
      "and: and\n",
      "its: it\n",
      "concepts: concept\n",
      ".: .\n",
      "``: ``\n",
      "lecture: lectur\n",
      "slides: slide\n",
      ",: ,\n",
      "explanations: explan\n",
      "were: were\n",
      "very: veri\n",
      "clear: clear\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "it: it\n",
      "&: &\n",
      "#: #\n",
      "039: 039\n",
      ";: ;\n",
      "s: s\n",
      "very: veri\n",
      "good: good\n",
      "to: to\n",
      "letting: let\n",
      "ask: ask\n",
      "questions: question\n",
      "and: and\n",
      "explain: explain\n",
      "again: again\n",
      "with: with\n",
      "suitable: suitabl\n",
      "examples: exampl\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "sometimes: sometim\n",
      ",: ,\n",
      "some: some\n",
      "codes: code\n",
      "on: on\n",
      "white: white\n",
      "board: board\n",
      "were: were\n",
      "unclear: unclear\n",
      "at: at\n",
      "the: the\n",
      "back: back\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "overall: overal\n",
      "very: veri\n",
      "good: good\n",
      "!: !\n",
      "!: !\n",
      "!: !\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "'': ''\n",
      "The: the\n",
      "lectures: lectur\n",
      "were: were\n",
      "good: good\n",
      "and: and\n",
      "clear: clear\n",
      ".: .\n",
      "And: and\n",
      "they: they\n",
      "weren: weren\n",
      "&: &\n",
      "#: #\n",
      "039: 039\n",
      ";: ;\n",
      "t: t\n",
      "too: too\n",
      "fast: fast\n",
      ".: .\n",
      "Writing: write\n",
      "code: code\n",
      "was: wa\n",
      "somewhat: somewhat\n",
      "confusing: confus\n",
      "because: becaus\n",
      "I: I\n",
      "didn: didn\n",
      "&: &\n",
      "#: #\n",
      "039: 039\n",
      ";: ;\n",
      "t: t\n",
      "know: know\n",
      "java: java\n",
      "before: befor\n",
      ".: .\n",
      "Actually: actual\n",
      "teaching: teach\n",
      "is: is\n",
      "very: veri\n",
      "good: good\n",
      "and: and\n",
      "can: can\n",
      "understand: understand\n",
      "easily: easili\n",
      "the: the\n",
      "concepts: concept\n",
      "by: by\n",
      "examples: exampl\n",
      "which: which\n",
      "are: are\n",
      "given: given\n",
      "in: in\n",
      "the: the\n",
      "class.it: class.it\n",
      "will: will\n",
      "be: be\n",
      "more: more\n",
      "helpful: help\n",
      "if: if\n",
      "provide: provid\n",
      "solved: solv\n",
      "questions: question\n",
      "as: as\n",
      "well: well\n",
      "!: !\n",
      ".: .\n",
      "thankyou: thankyou\n"
     ]
    }
   ],
   "source": [
    "feedback_words= feedbackTokens\n",
    "ps =PorterStemmer()\n",
    "for w in feedback_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(w + \": \" +rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting symspellpy\n",
      "  Downloading https://files.pythonhosted.org/packages/99/af/e71fcca6a42b6a63f518b0c1627e1f67822815cb0cf71e6af05acbd75c78/symspellpy-6.7.0-py3-none-any.whl (2.6MB)\n",
      "Requirement already satisfied: numpy>=1.13.1 in c:\\users\\rasika_105127\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from symspellpy) (1.15.4)\n",
      "Installing collected packages: symspellpy\n",
      "Successfully installed symspellpy-6.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
